{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L-WISE Data Analysis and Figure Generation\n",
    "\n",
    "This notebook generates (almost) all figure panels and performs all of the statistical analyses for the L-WISE paper. One exception to this is the generation of heatmaps for perturbations to example images, which is performed by imgproc_code/notebooks/visualize_heatmaps.ipynb.\n",
    "\n",
    "A de-identified version of all experimental data is being released for this project. The variable DEIDENTIFIED_DATA is \"True\" when using the de-identified dataset for the analysis. Setting it to \"False\" allows authors who have access to the raw source data to generate demographic tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEIDENTIFIED_DATA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "\n",
    "# Setting fonttype to the meaning of life makes text editable in exported pdfs\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "def get_df_from_xarray(data_paths, drop_columns=None):\n",
    "  start_pt_idx = 0\n",
    "  dfs = []\n",
    "  for data_path in data_paths: \n",
    "    assert os.path.isfile(data_path), f\"File {data_path} does not exist\"\n",
    "    ds = xr.open_dataset(data_path)\n",
    "    raw_df = ds.to_dataframe().reset_index()\n",
    "    df = raw_df[(raw_df['choice_slot'] == raw_df['i_choice']) | ((raw_df['i_choice'].isna()) & (raw_df['choice_slot'] == 0))]\n",
    "    df[\"participant\"] = start_pt_idx + df[\"participant\"]\n",
    "    start_pt_idx = start_pt_idx + df[\"participant\"].nunique()\n",
    "    dfs.append(df)\n",
    "\n",
    "  combined_df = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "  if drop_columns is not None:\n",
    "      combined_df = combined_df.drop(drop_columns, axis=1, errors='ignore')\n",
    "\n",
    "  # Sort dataframe such that trials for each participant appear in order\n",
    "  combined_df = combined_df.sort_values(by=['participant', 'obs'])\n",
    "\n",
    "  # Sort columns in a logical order\n",
    "  ordered_cols = ['participant', 'condition_idx', 'block', 'obs', 'trial_type', 'class', 'stimulus_image_url', 'stimulus_name', 'choice_name', 'i_correct_choice', 'i_choice', 'perf', 'reaction_time_msec', 'rel_timestamp_response', 'timestamp_start', 'monitor_width_px', 'monitor_height_px', 'stimulus_width_px', 'choice_width_px', 'stimulus_duration_msec', 'post_stimulus_delay_duration_msec', 'pre_choice_lockout_delay_duration_msec']\n",
    "  other_cols = [col for col in combined_df.columns if col not in ordered_cols]\n",
    "  combined_df = combined_df[ordered_cols + other_cols]\n",
    "\n",
    "  return combined_df\n",
    "\n",
    "\n",
    "# Function to calculate bootstrap confidence intervals\n",
    "def bootstrap_ci(data, num_bootstrap_samples=10000, confidence_level=0.95):\n",
    "    bootstrap_means = np.array([np.mean(np.random.choice(data, size=len(data), replace=True)) \n",
    "                                for _ in range(num_bootstrap_samples)])\n",
    "    return np.percentile(bootstrap_means, [(1 - confidence_level) / 2 * 100, (1 + confidence_level) / 2 * 100])\n",
    "\n",
    "\n",
    "def perform_chi_square(df, condition, control=0):\n",
    "    # Filter the dataframe for only the control and the specific condition\n",
    "    df_filtered = df[df['condition_idx'].isin([control, condition])]\n",
    "    \n",
    "    # Create a contingency table for the specific condition and control\n",
    "    contingency_table = pd.crosstab(df_filtered['condition_idx'], df_filtered['perf'])\n",
    "    \n",
    "    # Perform the Chi-square test\n",
    "    chi2, p, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "    \n",
    "    return chi2, p, dof, expected, contingency_table\n",
    "\n",
    "\n",
    "def chi_square_comparisons(df_trials_test, condition_idx_ordering, condition_labels, control_condition_idx=0):\n",
    "\n",
    "    control_group_name = condition_labels[condition_idx_ordering.index(control_condition_idx)]\n",
    "\n",
    "    # Perform chi-square tests for each condition compared to control\n",
    "    for label_idx, condition_idx in enumerate(condition_idx_ordering):\n",
    "        if condition_idx != control_condition_idx:\n",
    "            chi2, p, dof, expected, contingency_table = perform_chi_square(df_trials_test, condition_idx, control=control_condition_idx)\n",
    "            \n",
    "            print(f\"\\nComparing {condition_labels[label_idx]} to {control_group_name}:\")\n",
    "            print(f\"Chi-square value: {chi2:.4f}\")\n",
    "            print(f\"P-value: {p:.4f}\")\n",
    "            print(f\"Degrees of freedom: {dof}\")\n",
    "            \n",
    "            # Interpret the results\n",
    "            if p < 0.05:\n",
    "                print(f\"There is a significant difference in performance between {condition_labels[label_idx]} and {control_group_name}.\")\n",
    "            else:\n",
    "                print(f\"There is no significant difference in performance between {condition_labels[label_idx]} and {control_group_name}.\")\n",
    "            \n",
    "            # Display the contingency table\n",
    "            print(\"\\nContingency Table:\")\n",
    "            print(contingency_table)\n",
    "            \n",
    "            # Display expected frequencies\n",
    "            print(\"\\nExpected frequencies:\")\n",
    "            print(pd.DataFrame(expected, index=contingency_table.index, columns=contingency_table.columns))\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "\n",
    "def print_main_stats(df_trials, condition_idx_ordering, condition_labels, chance_level=0.25, test_blocks=[8,9]):\n",
    "  condition_accuracies = []\n",
    "  condition_cis = []\n",
    "\n",
    "  for cond in condition_idx_ordering:\n",
    "      cond_df = df_trials[(df_trials[\"condition_idx\"] == cond) & (df_trials[\"block\"].isin(test_blocks))]\n",
    "      mean_accuracy = cond_df['perf'].mean()\n",
    "      ci = bootstrap_ci(cond_df['perf'])\n",
    "      condition_accuracies.append(mean_accuracy)\n",
    "      condition_cis.append(ci)\n",
    "\n",
    "  # Create a DataFrame for plotting\n",
    "  condition_acc_data = pd.DataFrame({\n",
    "      'Condition': condition_labels,\n",
    "      'Accuracy': condition_accuracies,\n",
    "      'CI_lower': [ci[0] for ci in condition_cis],\n",
    "      'CI_upper': [ci[1] for ci in condition_cis]\n",
    "  })\n",
    "\n",
    "  # Calculate the error bars\n",
    "  condition_acc_data['yerr_lower'] = condition_acc_data['Accuracy'] - condition_acc_data['CI_lower']\n",
    "  condition_acc_data['yerr_upper'] = condition_acc_data['CI_upper'] - condition_acc_data['Accuracy']\n",
    "\n",
    "  print(\"Condition accuracies (mean with 95% CIs):\")\n",
    "  for condition, accuracy, ci in zip(condition_acc_data['Condition'], condition_acc_data['Accuracy'], condition_cis):\n",
    "      print(f\"{condition}: {accuracy:.2f} ({ci[0]:.2f}, {ci[1]:.2f})\")\n",
    "\n",
    "  # Calculate control accuracy (assumed to be the first condition)\n",
    "  control_accuracy = condition_acc_data['Accuracy'].iloc[0]\n",
    "\n",
    "  # Calculate and print percentage increase in margin above chance\n",
    "  print(\"\\nPercentage increase in margin above chance:\")\n",
    "  for condition, accuracy in zip(condition_acc_data['Condition'][1:], condition_acc_data['Accuracy'][1:]):  # Skip the first (control) condition\n",
    "      margin_control = control_accuracy - chance_level\n",
    "      margin_condition = accuracy - chance_level\n",
    "      \n",
    "      percentage_increase = ((margin_condition - margin_control) / margin_control) * 100\n",
    "\n",
    "      print(\"Condition acc:\", accuracy, \"| Control acc:\", control_accuracy)\n",
    "      print(\"Margin condition:\", margin_condition, \"| Margin control:\", margin_control)\n",
    "      print(f\"{condition}: {percentage_increase:.1f}%\")\n",
    "\n",
    "\n",
    "  # Calculate training time for each participant\n",
    "  df_trials_train = df_trials[~df_trials[\"block\"].isin(test_blocks)]\n",
    "  training_times = df_trials_train.groupby('participant')['rel_timestamp_response'].max().reset_index()\n",
    "  training_times = training_times.merge(df_trials_train[['participant', 'condition_idx']], on='participant', how='left')\n",
    "\n",
    "  # Function to calculate mean with bootstrap CI\n",
    "  def mean_with_ci(data, num_bootstrap_samples=10000, ci=0.95):\n",
    "      bootstrap_means = np.array([np.mean(np.random.choice(data, size=len(data), replace=True)) \n",
    "                                  for _ in range(num_bootstrap_samples)])\n",
    "      mean = np.mean(data)\n",
    "      ci_lower, ci_upper = np.percentile(bootstrap_means, [(1-ci)/2 * 100, (1+ci)/2 * 100])\n",
    "      return mean, ci_lower, ci_upper\n",
    "\n",
    "  # Calculate mean and CI for each condition\n",
    "  training_time_results = []\n",
    "  for c_idx, condition in enumerate(condition_idx_ordering):\n",
    "      if condition == 0 or condition:\n",
    "        condition_data = training_times[training_times['condition_idx'] == condition]['rel_timestamp_response']\n",
    "        mean, ci_lower, ci_upper = mean_with_ci(condition_data)\n",
    "        training_time_results.append({\n",
    "            'condition': condition_labels[c_idx],\n",
    "            'mean_training_time': round(mean/(1000*60), 4),\n",
    "            'ci_lower':  round(ci_lower/(1000*60), 4),\n",
    "            'ci_upper':  round(ci_upper/(1000*60), 4),\n",
    "        })\n",
    "\n",
    "  # Create a DataFrame with the results\n",
    "  training_time_results_df = pd.DataFrame(training_time_results)\n",
    "  print(\"Training times (minutes):\")\n",
    "  print(training_time_results_df)\n",
    "\n",
    "\n",
    "  # Calculate completion time for each participant\n",
    "  completion_times = df_trials.groupby('participant')['rel_timestamp_response'].max().reset_index()\n",
    "  completion_times = completion_times.merge(df_trials[['participant', 'condition_idx']], on='participant', how='left')\n",
    "\n",
    "  # Calculate mean and CI for each condition\n",
    "  completion_time_results = []\n",
    "  for c_idx, condition in enumerate(condition_idx_ordering):\n",
    "      if condition == 0 or condition:\n",
    "        condition_data = completion_times[completion_times['condition_idx'] == condition]['rel_timestamp_response']\n",
    "        mean, ci_lower, ci_upper = mean_with_ci(condition_data)\n",
    "        completion_time_results.append({\n",
    "            'condition': condition_labels[c_idx],\n",
    "            'mean_completion_time': round(mean/(1000*60), 4),\n",
    "            'ci_lower':  round(ci_lower/(1000*60), 4),\n",
    "            'ci_upper':  round(ci_upper/(1000*60), 4),\n",
    "        })\n",
    "\n",
    "  # Create a DataFrame with the results\n",
    "  completion_time_results_df = pd.DataFrame(completion_time_results)\n",
    "  print(\"Completion times (minutes):\")\n",
    "  print(completion_time_results_df)\n",
    "\n",
    "  return condition_acc_data, training_time_results_df, completion_time_results_df\n",
    "\n",
    "\n",
    "def assert_constant_counts(df):\n",
    "    # Print unique trial types for debugging\n",
    "    print(\"Unique trial types in the dataset:\", df['trial_type'].unique())\n",
    "    \n",
    "    # Group by trialset_id and get value counts for trial_type\n",
    "    counts = df.groupby(['experiment_id', 'trialset_id'])['trial_type'].value_counts().unstack(fill_value=0)\n",
    "    \n",
    "    # Get all unique trial types in the dataset\n",
    "    all_trial_types = ['calibration', 'repeat_stimulus']\n",
    "\n",
    "    for trial_type in all_trial_types:\n",
    "        if trial_type in counts.columns:\n",
    "            count_unique = counts[trial_type].nunique()\n",
    "            if count_unique != 1:\n",
    "                print(f\"\\nWarning: Count of {trial_type} is not constant across all trialset_ids\")\n",
    "                print(f\"Unique counts for {trial_type}: {counts[trial_type].unique()}\")\n",
    "            else:\n",
    "                print(f\"\\nCount of {trial_type} is constant ({counts[trial_type].iloc[0]}) across all trialset_ids\")\n",
    "        else:\n",
    "            print(f\"\\nWarning: Trial type '{trial_type}' is not present in counts DataFrame\")\n",
    "            print(\"This might indicate an issue with data processing\")\n",
    "\n",
    "    print(\"\\nAssertion check completed.\")\n",
    "\n",
    "\n",
    "def reassign_blocks(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Reassigns block values for specific participants in the dataframe based on observation numbers.\n",
    "    Only affects participants who have trials with 'shuffle' in their trial_type.\n",
    "    Includes verification of block sizes.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe containing columns 'participant', 'trial_type', 'obs', and 'block'\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with updated block values\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Find participants who have 'shuffle' in any of their trial_type values\n",
    "    shuffle_participants = df_copy[df_copy['trial_type'].str.contains('shuffle', na=False)]['participant'].unique()\n",
    "    \n",
    "    # Define the block structure\n",
    "    block_structure = {\n",
    "        0: 18,    # Block 0 has 18 trials\n",
    "        **{i: 19 for i in range(1, 8)},    # Blocks 1-7 have 19 trials each\n",
    "        **{i: 25 for i in range(8, 10)}    # Blocks 8-9 have 25 trials each\n",
    "    }\n",
    "\n",
    "    print(block_structure)\n",
    "    \n",
    "    # Calculate cumulative trial counts for block boundaries\n",
    "    cumulative_trials = [sum(block_structure[i] for i in range(k)) for k in range(len(block_structure) + 1)]\n",
    "    \n",
    "    # Function to assign block based on observation number\n",
    "    def get_block(obs):\n",
    "        for block_num, trial_boundary in enumerate(cumulative_trials[1:]):\n",
    "            if obs < trial_boundary:\n",
    "                return block_num\n",
    "        return len(block_structure) - 1  # Return last block number if beyond all boundaries\n",
    "    \n",
    "    # Process each participant who needs block reassignment\n",
    "    for participant in shuffle_participants:\n",
    "        # Get participant's data\n",
    "        mask = df_copy['participant'] == participant\n",
    "        participant_data = df_copy[mask].copy()\n",
    "        \n",
    "        # Sort by observation number\n",
    "        participant_data = participant_data.sort_values('obs')\n",
    "        \n",
    "        # Assign new block values based on observation position\n",
    "        df_copy.loc[mask, 'block'] = participant_data['obs'].apply(get_block)\n",
    "    \n",
    "    # Verify block sizes for each participant\n",
    "    print(\"\\nVerifying block sizes for each participant:\")\n",
    "    if verbose:\n",
    "        print(\"------------------------------------------\")\n",
    "    \n",
    "    all_correct = True\n",
    "    for participant in shuffle_participants:\n",
    "        participant_data = df_copy[df_copy['participant'] == participant]\n",
    "        \n",
    "        for block, expected_trials in block_structure.items():\n",
    "            block_trials = len(participant_data[participant_data['block'] == block])\n",
    "            \n",
    "            if block_trials != expected_trials:\n",
    "                print(f\"WARNING: Participant {participant} has {block_trials} trials in block {block} (expected {expected_trials})\")\n",
    "                all_correct = False\n",
    "            elif verbose:\n",
    "                print(f\"Participant {participant} has correct number of trials ({expected_trials}) in block {block}\")\n",
    "    \n",
    "    if all_correct:\n",
    "        print(\"\\nVERIFICATION PASSED: All participants have the correct number of trials in each block!\")\n",
    "    else:\n",
    "        print(\"\\nVERIFICATION FAILED: Some participants have incorrect numbers of trials in certain blocks.\")\n",
    "        \n",
    "    # Additional summary across all affected participants\n",
    "    if verbose:\n",
    "        print(\"\\nSummary across all participants with shuffled trials:\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "    for block, expected_trials in block_structure.items():\n",
    "        total_trials = sum(len(df_copy[(df_copy['participant'] == p) & (df_copy['block'] == block)]) \n",
    "                          for p in shuffle_participants)\n",
    "        num_participants = len(shuffle_participants)\n",
    "        if total_trials == expected_trials * num_participants:\n",
    "            if verbose:\n",
    "                print(f\"✓ Block {block}: All participants have exactly {expected_trials} trials\")\n",
    "        else:\n",
    "            print(f\"✗ Block {block}: Expected {expected_trials} trials per participant, \"\n",
    "                  f\"found {total_trials/num_participants:.1f} on average\")\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the current working directory to the parent directory (which contains the \"notebooks\" directory among others)\n",
    "changed_dir = False\n",
    "if not changed_dir and os.path.exists(\"./make_figs.ipynb\"):\n",
    "  os.chdir(os.path.dirname(os.getcwd()))\n",
    "  changed_dir = True\n",
    "assert os.path.exists(\"./notebooks/make_figs.ipynb\"), \"Make sure your working directory starts in 'notebooks'\"\n",
    "\n",
    "os.makedirs(\"notebooks/fig_outputs\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [\"stimulus_image_url_l\", \"stimulus_image_url_r\", \"class_l\", \"class_r\", \"mask_duration_msec\", \"mask_image_url\", \"choice_slot\", \"choice_image_urls\", \"keep_stimulus_on\", \"query_string\", \"platform\", \"bonus_usd_if_correct\"]\n",
    "if DEIDENTIFIED_DATA:\n",
    "  drop_columns.extend([\"assignment_id\", \"worker_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"idaea4\" moth learning task (from iNaturalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD IDAEA4 DATASET\n",
    "\n",
    "control_cond = \"natural\"\n",
    "trial_type_names_idaea4 = [control_cond, \"curriculum_sampling\", \"curriculum_sampling_shuffle\", \"enhancement_taper\", \"enhancement_taper_shuffle\", \"enhancement_taper_curriculum_sampling\"]\n",
    "\n",
    "if os.path.isfile(\"psych_data/df_idaea4.csv\") and DEIDENTIFIED_DATA:\n",
    "  print(\"Reading idaea4 dataset from saved .csv\")\n",
    "  df_idaea4 = pd.read_csv(\"psych_data/df_idaea4.csv\")\n",
    "else: # Load from .h5\n",
    "  print(\"Reading idaea4 dataset from .h5 file\")\n",
    "  \n",
    "  df_1 = get_df_from_xarray([\"./results/idaea4_learn_1_PARTIAL/idaea4_learn_1_combined_dataset.h5\"], drop_columns=drop_columns)\n",
    "  df_2 = get_df_from_xarray([\"./results/idaea4_learn_2/idaea4_learn_2_combined_dataset.h5\"], drop_columns=drop_columns)\n",
    "\n",
    "  df_1[\"experiment_id\"] = \"idaea4_learn_1\"\n",
    "  df_2[\"experiment_id\"] = \"idaea4_learn_2\"\n",
    "\n",
    "  df_2[\"participant\"] = df_2[\"participant\"] + df_1[\"participant\"].max() + 1\n",
    "\n",
    "  df_idaea4 = pd.concat([df_1, df_2])\n",
    "\n",
    "  condition_idx_remap = {\n",
    "    0: 0,\n",
    "    1: 0, \n",
    "    2: 1,\n",
    "    3: 2, \n",
    "    4: 3, \n",
    "    5: 4, \n",
    "    6: 5,\n",
    "  }\n",
    "  df_idaea4[\"condition_idx\"] = df_idaea4[\"condition_idx\"].map(condition_idx_remap).fillna(df_idaea4[\"condition_idx\"])\n",
    "\n",
    "  df_idaea4['perf'] = df_idaea4['perf'].fillna(0)\n",
    "\n",
    "  df_idaea4 = df_idaea4[df_idaea4[\"trialset_id\"]>0] \n",
    "\n",
    "  df_idaea4 = df_idaea4.sort_values(by=[\"participant\", \"rel_timestamp_response\"])\n",
    "\n",
    "  df_idaea4 = df_idaea4.reset_index(drop=True)\n",
    "\n",
    "  df_idaea4 = reassign_blocks(df_idaea4, verbose=False)\n",
    "\n",
    "  if DEIDENTIFIED_DATA:\n",
    "    print(\"Saving de-identified version of the dataset\")\n",
    "    df_idaea4.to_csv(\"psych_data/df_idaea4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILTER OUT GUESSING PARTICIPANTS IN IDAEA4\n",
    "\n",
    "df_idaea4_trials = df_idaea4[df_idaea4[\"trial_type\"].isin(trial_type_names_idaea4)]\n",
    "\n",
    "df_idaea4_calib = df_idaea4[df_idaea4[\"stimulus_name\"].isin([\"circle\", \"triangle\"])]\n",
    "\n",
    "calib_means = df_idaea4_calib.groupby(\"participant\")[\"perf\"].mean()\n",
    "\n",
    "# Filter out participants with a mean calibration 'perf' of less than 0.9\n",
    "participants_calib_above09 = calib_means[calib_means >= 0.9].index\n",
    "\n",
    "print(f\"All participants: {df_idaea4['participant'].nunique()}\")\n",
    "\n",
    "# Filter the DataFrame for these participants\n",
    "df_idaea4 = df_idaea4[df_idaea4['participant'].isin(participants_calib_above09)]\n",
    "\n",
    "print(f\"Participants with calib acc of 0.9 and above: {df_idaea4['participant'].nunique()}\")\n",
    "\n",
    "df_idaea4_trials = df_idaea4_trials[df_idaea4_trials['participant'].isin(participants_calib_above09)]\n",
    "\n",
    "df_idaea4_trials_test = df_idaea4_trials[df_idaea4_trials[\"block\"].isin([8, 9])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moth task learning performance statistics (Table 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_idx_ordering_idaea4 = [0, 3, 4, 1, 2, 5]\n",
    "condition_labels_idaea4 = [\"Control\", \"Enhance\", \"Enhance (shuffle)\", \"Select\", \"Select (shuffle)\", \"L-WISE\"]\n",
    "\n",
    "idaea4_accuracy_df, idaea4_training_time_df, idaea4_completion_time_df = print_main_stats(df_idaea4_trials, condition_idx_ordering_idaea4, condition_labels_idaea4, chance_level=0.25, test_blocks=[8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_square_comparisons(df_idaea4_trials_test, condition_idx_ordering_idaea4, condition_labels_idaea4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We re-run the analysis with L-WISE (condition number 5) as the \"control\"\n",
    "\n",
    "chi_square_comparisons(df_idaea4_trials_test, condition_idx_ordering_idaea4, condition_labels_idaea4, control_condition_idx=condition_labels_idaea4.index(\"L-WISE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig. 3B (learning curves for moth task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT IDAEA4 LEARNING CURVES\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "def get_difficulty(image_path, dirmap):\n",
    "    image_name = image_path.split('/')[-1].split('?')[0]\n",
    "    difficulty = dirmap[dirmap['im_path'].str.contains(image_name)]['difficulty'].values\n",
    "    return difficulty[0] if len(difficulty) > 0 else np.nan\n",
    "\n",
    "def get_difficulty_percentile(image_path, dirmap):\n",
    "    image_name = image_path.split('/')[-1].split('?')[0]\n",
    "    row = dirmap[dirmap['im_path'].str.contains(image_name)]\n",
    "    \n",
    "    if row.empty:\n",
    "        return np.nan\n",
    "    \n",
    "    difficulty = row['difficulty'].values[0]\n",
    "    class_value = row['class'].values[0]\n",
    "    split_value = row['split'].values[0]\n",
    "    \n",
    "    # Filter the dirmap for the specific class and split\n",
    "    subset = dirmap[(dirmap['class'] == class_value) & (dirmap['split'] == split_value)]\n",
    "    \n",
    "    # Calculate the percentile\n",
    "    percentile = (subset['difficulty'] <= difficulty).mean() * 100\n",
    "    \n",
    "    return percentile\n",
    "\n",
    "def get_enhancement(url):\n",
    "    if 'natural' in url:\n",
    "        return 0\n",
    "    parts = url.split('-')\n",
    "    for part in parts:\n",
    "        if 'dot' in part:\n",
    "            part = part.replace('dot', '.')\n",
    "        try:\n",
    "            return float(part)\n",
    "        except:\n",
    "            pass\n",
    "    return 0\n",
    "\n",
    "def normalize_enhancement(enhancement, max_enhancement):\n",
    "    if max_enhancement == 0:\n",
    "        return 0\n",
    "    return enhancement / max_enhancement\n",
    "\n",
    "def add_trial_index(df):\n",
    "    #df = df.sort_values(['participant', 'block', 'trial'])\n",
    "    df['trial_index'] = df.groupby('participant').cumcount() + 1\n",
    "    return df\n",
    "\n",
    "def smooth_with_forced_start(x, window_length, polyorder, force_points=1):\n",
    "    smoothed = savgol_filter(x, window_length, polyorder, mode='nearest')\n",
    "    smoothed[:force_points] = x[:force_points]\n",
    "    return smoothed\n",
    "\n",
    "def smooth_performance(df):\n",
    "    df['smoothed_perf'] = df.groupby('participant')['perf'].transform(\n",
    "        lambda x: smooth_with_forced_start(x, window_length=31, polyorder=2, force_points=1)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def process_condition_data(df, condition, dirmap):\n",
    "    df_cond = df[df[\"condition_idx\"] == condition].copy()\n",
    "    df_cond['difficulty'] = df_cond['stimulus_image_url'].apply(lambda x: get_difficulty_percentile(x, dirmap))\n",
    "    df_cond['enhancement'] = df_cond['stimulus_image_url'].apply(get_enhancement)\n",
    "    \n",
    "    # Group by trial index and calculate means across participants\n",
    "    grouped = df_cond.groupby('trial_index')\n",
    "    avg_data = pd.DataFrame({\n",
    "        'perf': grouped['smoothed_perf'].mean(),\n",
    "        'perf_sem': grouped['smoothed_perf'].sem(),\n",
    "        'difficulty': grouped['difficulty'].mean(),\n",
    "        'enhancement': grouped['enhancement'].mean()\n",
    "    })\n",
    "    \n",
    "    return avg_data\n",
    "\n",
    "def create_comparison_plot(df_trials, dirmap, control_condition, condition, trial_type_names, chance_level=0.25, ylim=[0,1], max_enhance_eps=8):\n",
    "    cond_dict = {idx: name for idx, name in enumerate(trial_type_names)}\n",
    "    \n",
    "    df_trials = add_trial_index(df_trials)\n",
    "    df_trials = smooth_performance(df_trials)\n",
    "    \n",
    "    trial_counts = df_trials.groupby('participant')['trial_index'].max()\n",
    "    print(f\"Minimum trial count: {trial_counts.min()}\")\n",
    "    print(f\"Maximum trial count: {trial_counts.max()}\")\n",
    "    \n",
    "    control_data = process_condition_data(df_trials, control_condition, dirmap)\n",
    "    condition_data = process_condition_data(df_trials, condition, dirmap)\n",
    "    \n",
    "    max_trial = max(control_data.index.max(), condition_data.index.max())\n",
    "    \n",
    "    plt.style.use('seaborn-v0_8-white')\n",
    "    sns.set_palette(\"deep\")\n",
    "\n",
    "    # Create figure with three subplots\n",
    "    fig, (ax_top, ax_middle, ax_bottom) = plt.subplots(3, 1, figsize=(9, 7), height_ratios=[0.75, 0.75, 2.75], sharex=True)\n",
    "    \n",
    "    # Top subplot for enhancement (epsilon)\n",
    "    sns.lineplot(x=control_data.index, y=control_data['enhancement'], ax=ax_top, label='Control ϵ', color='black', linestyle=':', linewidth=1)\n",
    "    sns.lineplot(x=condition_data.index, y=condition_data['enhancement'], ax=ax_top, label='L-WISE ϵ', color='red', linewidth=1)\n",
    "    \n",
    "    ax_top.set_ylabel('Enhance ϵ', fontsize=14, fontweight='bold')\n",
    "    ax_top.set_ylim(-max_enhance_eps/40, max_enhance_eps + (max_enhance_eps/40))\n",
    "    ax_top.set_yticks([0, max_enhance_eps/2, max_enhance_eps])\n",
    "    ax_top.legend(loc='lower right', facecolor='white', fontsize=14)\n",
    "\n",
    "    ax_top.spines['top'].set_visible(False)\n",
    "    ax_top.spines['right'].set_visible(False)   \n",
    "\n",
    "    # Middle subplot for difficulty\n",
    "    sns.lineplot(x=control_data.index, y=control_data['difficulty'], ax=ax_middle, label='Control Difficulty', color='black', linestyle=':', linewidth=1)\n",
    "    sns.lineplot(x=condition_data.index, y=condition_data['difficulty'], ax=ax_middle, label='L-WISE Difficulty', color='red', linewidth=1)\n",
    "    \n",
    "    ax_middle.set_ylabel('Diff. %ile', fontsize=14, fontweight='bold')\n",
    "    ax_middle.set_ylim(0, 80)\n",
    "    ax_middle.set_yticks([0, 40, 80])\n",
    "    ax_middle.legend(loc='lower right', facecolor='white', fontsize=14)\n",
    "\n",
    "    ax_middle.spines['top'].set_visible(False)\n",
    "    ax_middle.spines['right'].set_visible(False)   \n",
    "\n",
    "    # Bottom subplot for performance (keep this the same as before)\n",
    "    sns.lineplot(x=control_data.index, y=control_data['perf'], ax=ax_bottom, label=f'Control Group', color='black', linestyle=':', linewidth=2)\n",
    "    ax_bottom.fill_between(control_data.index, \n",
    "                           control_data['perf'] - control_data['perf_sem'], \n",
    "                           control_data['perf'] + control_data['perf_sem'], \n",
    "                           color='black', alpha=0.2)\n",
    "    \n",
    "    sns.lineplot(x=condition_data.index, y=condition_data['perf'], ax=ax_bottom, label=f'{cond_dict[condition]} Group', color='red', linewidth=2)\n",
    "    ax_bottom.fill_between(condition_data.index, \n",
    "                           condition_data['perf'] - condition_data['perf_sem'], \n",
    "                           condition_data['perf'] + condition_data['perf_sem'], \n",
    "                           color='red', alpha=0.2)\n",
    "    \n",
    "    ax_bottom.set_xlabel('Trial', fontsize=14, fontweight='bold')\n",
    "    ax_bottom.set_ylabel('Accuracy (original groundtruth)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Set x-axis range\n",
    "    ax_bottom.set_xlim(0, max_trial)\n",
    "    ax_bottom.set_ylim(*ylim)\n",
    "    \n",
    "    # Add vertical and horizontal lines\n",
    "    ax_bottom.axvline(x=128, color='grey', linestyle='--', linewidth=1)\n",
    "    ax_bottom.axhline(y=chance_level, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "    ax_bottom.spines['top'].set_visible(False)\n",
    "    ax_bottom.spines['right'].set_visible(False)   \n",
    "    \n",
    "    ax_bottom.legend(loc='upper right', facecolor='white', fontsize=14)\n",
    "    \n",
    "    # Adjust tick label sizes\n",
    "    for ax in [ax_top, ax_middle, ax_bottom]:\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "    compare_title = f\"{cond_dict[condition]}\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"notebooks/fig_outputs/{df_trials.iloc[0][\"experiment_id\"].split(\"_\")[0]}_{compare_title}.pdf\", dpi=300, format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_condition = 0\n",
    "dirmap_idaea4 = pd.read_csv('psych_data/dataset_dirmaps/idaea4_dataset_dirmap.csv')\n",
    "create_comparison_plot(df_idaea4_trials, dirmap_idaea4, control_condition, trial_type_names_idaea4.index(\"enhancement_taper_curriculum_sampling\"), trial_type_names_idaea4, chance_level=0.25, ylim=[0.15, 0.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix Fig. S14A1: (relationship between human accuracy and ground truth logit in moth task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_robust_gt_logit(image_path, dirmap):\n",
    "    image_name = image_path.split('/')[-1].split('?')[0]\n",
    "    robust_gt_logit = dirmap[dirmap['im_path'].str.contains(image_name)]['robust_gt_logit'].values\n",
    "    return robust_gt_logit[0] if len(robust_gt_logit) > 0 else np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from scipy.stats import pearsonr\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def analyze_logit_accuracy_relationship(df_trials, dirmap, test_blocks=None, trial_type_names=None, ylim=[0,1], xlim=None, control_only=False, bins=10, print_num_data_points=False, retrieve_logits=True, fig_file_name=None, fig_size=(3,3), all_black=False):\n",
    "    \"\"\"Analyze relationship between robust_gt_logit and participant accuracy\"\"\"\n",
    "    plt.style.use('seaborn-v0_8-white')\n",
    "    plt.rcParams['xtick.bottom'] = True\n",
    "    plt.rcParams['ytick.left'] = True\n",
    "\n",
    "    # Create mask for filtering\n",
    "    mask = pd.Series(True, index=df_trials.index)\n",
    "    \n",
    "    if test_blocks is not None:\n",
    "        mask &= df_trials['block'].isin(test_blocks)\n",
    "    \n",
    "    if trial_type_names is not None:\n",
    "        mask &= df_trials['trial_type'].isin(trial_type_names)\n",
    "    \n",
    "    if control_only:\n",
    "        mask &= df_trials['condition_idx'] == 0\n",
    "    \n",
    "    # Apply filter\n",
    "    df_test = df_trials[mask].copy()\n",
    "    \n",
    "    if retrieve_logits:\n",
    "        df_test['robust_gt_logit'] = df_test['stimulus_image_url'].apply(\n",
    "            lambda x: get_robust_gt_logit(x, dirmap)\n",
    "        )\n",
    "    \n",
    "    # Remove outliers (beyond 3 std from mean)\n",
    "    difficulty_mean = df_test['robust_gt_logit'].mean()\n",
    "    difficulty_std = df_test['robust_gt_logit'].std()\n",
    "    df_test = df_test[\n",
    "        (df_test['robust_gt_logit'] >= difficulty_mean - 3*difficulty_std) & \n",
    "        (df_test['robust_gt_logit'] <= difficulty_mean + 3*difficulty_std)\n",
    "    ]\n",
    "    \n",
    "    # Create difficulty bins\n",
    "    df_test['robust_gt_logit_bin'] = pd.cut(df_test['robust_gt_logit'], bins=bins)\n",
    "    \n",
    "    # Calculate statistics for each bin\n",
    "    binned_data = df_test.groupby('robust_gt_logit_bin').agg({\n",
    "        'robust_gt_logit': ['mean', 'count', 'std'],\n",
    "        'perf': ['mean', lambda x: bootstrap_ci(x)[0], lambda x: bootstrap_ci(x)[1]]\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    binned_data.columns = ['robust_gt_logit_bin', 'bin_center', 'count', 'robust_gt_logit_std', \n",
    "                          'perf_mean', 'perf_ci_lower', 'perf_ci_upper']\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=fig_size, dpi=300)\n",
    "    \n",
    "    # Plot error bars\n",
    "    errorbar_params = {\n",
    "        'fmt': 'o', \n",
    "        'capsize': 3, \n",
    "        'capthick': 1.5, \n",
    "        'elinewidth': 1.5, \n",
    "        'markersize': 4, \n",
    "        'zorder': 2\n",
    "    }\n",
    "    if all_black:\n",
    "        errorbar_params['ecolor'] = 'black'\n",
    "        errorbar_params['color'] = 'black'\n",
    "    ax.errorbar(binned_data['bin_center'], \n",
    "                binned_data['perf_mean'],\n",
    "                xerr=binned_data['robust_gt_logit_std'],\n",
    "                yerr=[(binned_data['perf_mean'] - binned_data['perf_ci_lower']),  # lower error\n",
    "                      (binned_data['perf_ci_upper'] - binned_data['perf_mean'])], # upper error\n",
    "                **errorbar_params)\n",
    "    \n",
    "    # Add count labels if requested\n",
    "    if print_num_data_points:\n",
    "        for x, y, count, yerr in zip(binned_data['bin_center'], \n",
    "                                   binned_data['perf_mean'],\n",
    "                                   binned_data['count'],\n",
    "                                   binned_data['perf_ci_upper'] - binned_data['perf_mean']):\n",
    "            # Position text slightly above the error bar\n",
    "            text_y = y + yerr + 0.02  # Adjust the 0.02 offset as needed\n",
    "            ax.text(x, text_y, int(count), \n",
    "                   ha='center', va='bottom',\n",
    "                   fontsize=8)  # Adjust fontsize as needed\n",
    "    \n",
    "    # Fit logistic regression (use statsmodels to get p value)\n",
    "    X = df_test['robust_gt_logit'].values.reshape(-1, 1)\n",
    "    y = df_test['perf']\n",
    "    \n",
    "    X_sm = sm.add_constant(X)\n",
    "    logit_model_sm = sm.Logit(y, X_sm).fit()\n",
    "    print(logit_model_sm.summary())\n",
    "    p_value = logit_model_sm.pvalues[1]\n",
    "\n",
    "    log_reg = LogisticRegression().fit(X, y)\n",
    "    \n",
    "    # Generate points for the logistic curve\n",
    "    X_plot = np.linspace(\n",
    "        difficulty_mean - 3*difficulty_std,\n",
    "        difficulty_mean + 3*difficulty_std,\n",
    "        500\n",
    "    ).reshape(-1, 1)\n",
    "    y_plot = log_reg.predict_proba(X_plot)[:, 1]\n",
    "    \n",
    "    # Plot logistic regression curve\n",
    "    plt.plot(X_plot, y_plot, 'k-', label='Logistic Regression Fit', zorder=1)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_xlabel('Ground truth logit', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylim(ylim)\n",
    "    if xlim is not None:\n",
    "        ax.set_xlim(xlim)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "    # Calculate AUC\n",
    "    auc = roc_auc_score(y, log_reg.predict_proba(X)[:, 1])\n",
    "    \n",
    "    # Calculate AUC with 10-fold cross-validation\n",
    "    cv_auc_scores = cross_val_score(\n",
    "        LogisticRegression(), \n",
    "        X, \n",
    "        y, \n",
    "        cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42),\n",
    "        scoring='roc_auc'\n",
    "    )\n",
    "    cv_auc_mean = np.mean(cv_auc_scores)\n",
    "    cv_auc_std = np.std(cv_auc_scores)\n",
    "    \n",
    "    # Remove top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Make remaining spines thicker\n",
    "    ax.spines['left'].set_linewidth(2)\n",
    "    ax.spines['bottom'].set_linewidth(2)\n",
    "\n",
    "    # Add these lines to customize tick marks\n",
    "    ax.tick_params(axis='both', which='major', length=6, width=2)\n",
    "    ax.tick_params(axis='both', which='minor', length=3, width=1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    if fig_file_name:\n",
    "        fname = fig_file_name\n",
    "    else:\n",
    "        fname = f\"{df_trials.iloc[0]['experiment_id'].split('_')[0]}_robust_gt_logit_accuracy.pdf\"\n",
    "    plt.savefig(os.path.join(\"notebooks/fig_outputs\", fname), dpi=300, format=\"pdf\", bbox_inches=\"tight\")\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"P-value: {p_value:.3e}\")\n",
    "    print(f\"AUC score: {auc:.3f}\")\n",
    "    print(f\"Cross-validated AUC: {cv_auc_mean:.3f} ± {cv_auc_std:.3f}\")\n",
    "    \n",
    "    return fig, ax, {\n",
    "        'p_value': p_value,\n",
    "        'auc': auc,\n",
    "        'binned_data': binned_data,\n",
    "        'logistic_regression': log_reg\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_logit_accuracy_relationship(df_idaea4_trials, dirmap_idaea4, test_blocks=[8,9], trial_type_names=trial_type_names_idaea4, ylim=[0.2,0.7], control_only=True, bins=6, print_num_data_points=True, all_black=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix Fig. S14A2: (relationship between human accuracy and enhancement epsilon in moth task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def analyze_enhancement_accuracy_relationship(df_trials, dirmap, condition_idx=None, test_blocks=[2,3,4,5], trial_type_names=None, min_acc=None, ylim=[0,1]):\n",
    "    \"\"\"Analyze relationship between image enhancement level and participant accuracy\"\"\"\n",
    "    plt.style.use('seaborn-v0_8-white')\n",
    "    plt.rcParams['xtick.bottom'] = True\n",
    "    plt.rcParams['ytick.left'] = True\n",
    "    \n",
    "    # Filter for test blocks, specified trial types, and condition\n",
    "    df_test = df_trials[\n",
    "        (df_trials['block'].isin(test_blocks)) &\n",
    "        (df_trials['trial_type'].isin(trial_type_names) if trial_type_names else True)\n",
    "    ].copy()\n",
    "\n",
    "    if min_acc is not None:\n",
    "        # Calculate mean accuracy per participant\n",
    "        participant_accs = df_test.groupby('participant')['perf'].mean()\n",
    "        # Get list of participants meeting minimum accuracy criterion\n",
    "        include_participants = participant_accs[participant_accs >= min_acc].index\n",
    "        # Filter dataframe to keep only those participants\n",
    "        df_test = df_test[df_test['participant'].isin(include_participants)]\n",
    "\n",
    "    print(len(df_test), \"total trials included in analysis\")\n",
    "    \n",
    "    # Additional filtering for condition if specified\n",
    "    if condition_idx is not None:\n",
    "        df_test = df_test[df_test['condition_idx'] == condition_idx]\n",
    "    \n",
    "    # Get enhancement epsilon for each trial\n",
    "    df_test['enhance_eps'] = df_test['stimulus_image_url'].apply(\n",
    "        lambda x: get_enhancement(x)\n",
    "    )\n",
    "\n",
    "    print(list(df_test[\"enhance_eps\"]))\n",
    "    \n",
    "    # Calculate statistics for each enhancement level\n",
    "    grouped_data = df_test.groupby('enhance_eps').agg({\n",
    "        'perf': ['mean', 'count', lambda x: bootstrap_ci(x)[0], lambda x: bootstrap_ci(x)[1]]\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    grouped_data.columns = ['enhance_eps', 'perf_mean', 'count', 'perf_ci_lower', 'perf_ci_upper']\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(3, 3), dpi=300)\n",
    "    \n",
    "    # Plot error bars for each discrete enhancement level\n",
    "    ax.errorbar(grouped_data['enhance_eps'], \n",
    "                grouped_data['perf_mean'],\n",
    "                yerr=[(grouped_data['perf_mean'] - grouped_data['perf_ci_lower']),  # lower error\n",
    "                      (grouped_data['perf_ci_upper'] - grouped_data['perf_mean'])], # upper error\n",
    "                fmt='o', capsize=3, capthick=1.5, elinewidth=1.5, markersize=4, color='black', ecolor='black')\n",
    "    \n",
    "    # Fit logistic regression (use statsmodels to get p value)\n",
    "    X = df_test['enhance_eps'].values.reshape(-1, 1)\n",
    "    y = df_test['perf']\n",
    "    \n",
    "    log_reg = LogisticRegression().fit(X, y)\n",
    "\n",
    "    X_sm = sm.add_constant(X)\n",
    "    logit_model_sm = sm.Logit(y, X_sm).fit()\n",
    "    print(logit_model_sm.summary())\n",
    "    p_value = logit_model_sm.pvalues[1]\n",
    "    \n",
    "    # Generate points for the logistic curve\n",
    "    X_plot = np.linspace(\n",
    "        grouped_data['enhance_eps'].min(),\n",
    "        grouped_data['enhance_eps'].max(),\n",
    "        500\n",
    "    ).reshape(-1, 1)\n",
    "    y_plot = log_reg.predict_proba(X_plot)[:, 1]\n",
    "    \n",
    "    # Plot logistic regression curve\n",
    "    plt.plot(X_plot, y_plot, 'k-', label='Logistic Regression Fit')\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_xlabel('Enhancement ϵ', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(ylim)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "    # Calculate AUC\n",
    "    auc = roc_auc_score(y, log_reg.predict_proba(X)[:, 1])\n",
    "    \n",
    "    # Remove top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Make remaining spines thicker\n",
    "    ax.spines['left'].set_linewidth(2)\n",
    "    ax.spines['bottom'].set_linewidth(2)\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', length=6, width=2)\n",
    "    ax.tick_params(axis='both', which='minor', length=3, width=1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    save_name = df_trials.iloc[0]['experiment_id'].split('_')[0]\n",
    "    if condition_idx is not None:\n",
    "        save_name += f\"_condition{condition_idx}\"\n",
    "    plt.savefig(f\"notebooks/fig_outputs/{save_name}_enhancement_accuracy.pdf\", \n",
    "                dpi=300, format=\"pdf\", bbox_inches=\"tight\")\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"P-value: {p_value:.3e}\")\n",
    "    print(f\"AUC score: {auc:.3f}\")\n",
    "    \n",
    "    return fig, ax, {\n",
    "        'p_value': p_value,\n",
    "        'auc': auc,\n",
    "        'grouped_data': grouped_data,\n",
    "        'logistic_regression': log_reg\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_enhancement_accuracy_relationship(df_idaea4_trials, dirmap_idaea4, test_blocks=[0, 1, 2, 3, 4, 5], trial_type_names=trial_type_names_idaea4, condition_idx=4, min_acc=None, ylim=[0.3,0.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix Fig. S15 (analyzing the effect of Greek name alias assignment on participant accuracy in the moth task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SUPPLEMENTARY CLASS MAPPING ANALYSIS PLOTS FOR IDAEA4\n",
    "\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "def analyze_class_mapping(df, class_name, stimuli=['Ajax', 'Eris', 'Leda', 'Tyro']):\n",
    "    def get_class_stimulus(participant_data):\n",
    "        mask = participant_data['class'] == class_name\n",
    "        return participant_data.loc[mask, 'stimulus_name'].iloc[0]\n",
    "    \n",
    "    class_mappings = df.groupby('participant').apply(get_class_stimulus)\n",
    "    \n",
    "    def calculate_z_score(group, condition_means, condition_stds):\n",
    "        condition = group['condition_idx'].iloc[0]\n",
    "        avg_perf = group['perf'].mean()\n",
    "        z_score = (avg_perf - condition_means[condition]) / condition_stds[condition]\n",
    "        return z_score\n",
    "\n",
    "    condition_stats = df.groupby('condition_idx')['perf'].agg(['mean', 'std'])\n",
    "    condition_means, condition_stds = condition_stats['mean'].to_dict(), condition_stats['std'].to_dict()\n",
    "    \n",
    "    accuracy_z_scores = df.groupby('participant').apply(calculate_z_score, condition_means, condition_stds)\n",
    "    \n",
    "    analysis_df = pd.DataFrame({\n",
    "        'stimulus': class_mappings,\n",
    "        'z_score': accuracy_z_scores,\n",
    "        'class': class_name\n",
    "    })\n",
    "    \n",
    "    groups = [group['z_score'].values for name, group in analysis_df.groupby('stimulus') if name in stimuli]\n",
    "    f_statistic, p_value = stats.f_oneway(*groups)\n",
    "    \n",
    "    df_between = len(groups) - 1\n",
    "    df_within = len(analysis_df) - len(groups)\n",
    "    eta_squared = (f_statistic * df_between) / (f_statistic * df_between + df_within)\n",
    "    \n",
    "    return analysis_df, f_statistic, p_value, eta_squared, df_between, df_within\n",
    "\n",
    "\n",
    "def class_mapping_plots(df_trials_test, classes, class_names):\n",
    "    # Analyze all classes\n",
    "\n",
    "    results = {cls: analyze_class_mapping(df_trials_test, cls) for cls in classes}\n",
    "\n",
    "    # Combine all results into a single DataFrame\n",
    "    combined_df = pd.concat([df for df, _, _, _, _, _ in results.values()])\n",
    "    combined_df['class_name'] = combined_df['class'].map(class_names)\n",
    "\n",
    "    # Set up the plot style\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    sns.set_palette(\"deep\")\n",
    "    plt.figure(figsize=(12, 12))\n",
    "\n",
    "    # Create the plot\n",
    "    ax = sns.boxplot(x='class_name', y='z_score', hue='stimulus', data=combined_df,\n",
    "                    width=0.7, fliersize=3, linewidth=2)\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.xlabel(None)\n",
    "    plt.ylabel('Condition-Normalized Accuracy (Z-Score)', fontsize=28)\n",
    "    plt.xticks(fontsize=24)\n",
    "    plt.yticks(fontsize=24)\n",
    "\n",
    "    # Adjust y-axis limits to make boxplots smaller\n",
    "    y_min, y_max = plt.ylim()\n",
    "    plt.ylim(-0.7, 0.75)\n",
    "\n",
    "    # Add ANOVA results below each group\n",
    "    for i, cls in enumerate(classes):\n",
    "        df, f_stat, p_val, eta_sq, df_between, df_within = results[cls]\n",
    "        stats_text = f\"F({df_between},{df_within})={f_stat:.2f}\\np={p_val:.4f}\\nη²={eta_sq:.3f}\"\n",
    "        plt.text(i, plt.ylim()[1]-0.01, stats_text, ha='center', va='top', fontsize=20, \n",
    "                bbox=dict(facecolor='white', edgecolor='none', alpha=0.8))\n",
    "\n",
    "    # Adjust the subplot to make room for the legend\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.2)\n",
    "\n",
    "    # Get the handles and labels from the main plot\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    # Create the legend below the plot\n",
    "    plt.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, -0.08),\n",
    "            ncol=4, fontsize=26, title='Alias', title_fontsize=28)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"notebooks/fig_outputs/{df_trials_test.iloc[0][\"experiment_id\"].split(\"_\")[0]}_class_mapping_analysis.pdf\", dpi=300, bbox_inches='tight', format='pdf')\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    for cls, (df, f_stat, p_val, eta_sq, df_between, df_within) in results.items():\n",
    "        print(f\"\\nANOVA results for '{cls}' mapping:\")\n",
    "        print(f\"F({df_between},{df_within}) = {f_stat:.4f}\")\n",
    "        print(f\"p-value: {p_val:.4f}\")\n",
    "        print(f\"Effect size (η²): {eta_sq:.4f}\")\n",
    "        print(\"\\nDescriptive statistics:\")\n",
    "        print(df.groupby('stimulus')['z_score'].describe())\n",
    "\n",
    "        if p_val < 0.05:\n",
    "            print(\"\\nTukey's HSD post-hoc test:\")\n",
    "            tukey_results = pairwise_tukeyhsd(df['z_score'], df['stimulus'])\n",
    "            print(tukey_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['01233_Animalia_Arthropoda_Insecta_Lepidoptera_Geometridae_Idaea_aversata', '01234_Animalia_Arthropoda_Insecta_Lepidoptera_Geometridae_Idaea_biselata', '01239_Animalia_Arthropoda_Insecta_Lepidoptera_Geometridae_Idaea_seriata', '01240_Animalia_Arthropoda_Insecta_Lepidoptera_Geometridae_Idaea_tacturata']\n",
    "class_names = {\n",
    "    '01233_Animalia_Arthropoda_Insecta_Lepidoptera_Geometridae_Idaea_aversata': 'aversata',\n",
    "    '01234_Animalia_Arthropoda_Insecta_Lepidoptera_Geometridae_Idaea_biselata': 'biselata',\n",
    "    '01239_Animalia_Arthropoda_Insecta_Lepidoptera_Geometridae_Idaea_seriata': 'seriata',\n",
    "    '01240_Animalia_Arthropoda_Insecta_Lepidoptera_Geometridae_Idaea_tacturata': 'tacturata'\n",
    "}\n",
    "class_mapping_plots(df_idaea4_trials_test, classes, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAM10000 dermoscopy learning task (4-class version \"ham4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD HAM4 DATA\n",
    "\n",
    "control_cond = \"natural\"\n",
    "trial_type_names_ham4 = [control_cond, \"curriculum_sampling\", \"curriculum_sampling_shuffle\", \"enhancement_taper\", \"enhancement_taper_shuffle\", \"enhancement_taper_curriculum_sampling\"]\n",
    "\n",
    "if os.path.isfile(\"psych_data/df_ham4.csv\") and DEIDENTIFIED_DATA:\n",
    "  print(\"Reading ham4 dataset from saved .csv\")\n",
    "  df_ham4 = pd.read_csv(\"psych_data/df_ham4.csv\")\n",
    "else: # Load from .h5\n",
    "  print(\"Reading ham4 dataset from .h5 file\")\n",
    "\n",
    "  df_ham4_learn_4 = get_df_from_xarray([\"./results/ham4_learn_4/ham4_learn_4_combined_dataset.h5\"], drop_columns=drop_columns)\n",
    "\n",
    "  # Remap condition indices and trial types (in the first deployment of the experiment, the shuffling procedure was not performed at all due to a bug)\n",
    "  condition_idx_remap = {\n",
    "    2: 1, \n",
    "    4: 3,\n",
    "  }\n",
    "  trial_type_remap = {\n",
    "    \"curriculum_sampling_shuffle\": \"curriculum_sampling\",\n",
    "    \"enhancement_taper_shuffle\": \"enhancement_taper\",\n",
    "  }\n",
    "  df_ham4_learn_4[\"condition_idx\"] = df_ham4_learn_4[\"condition_idx\"].map(condition_idx_remap).fillna(df_ham4_learn_4[\"condition_idx\"])\n",
    "  df_ham4_learn_4[\"trial_type\"] = df_ham4_learn_4[\"trial_type\"].map(trial_type_remap).fillna(df_ham4_learn_4[\"trial_type\"])\n",
    "  df_ham4_learn_4[\"data_round\"] = 4\n",
    "\n",
    "  df_ham4_learn_4[\"experiment_id\"] = \"ham4_learn_4\"\n",
    "\n",
    "  df_ham4_learn_5 = get_df_from_xarray([\"./results/ham4_learn_5/ham4_learn_5_combined_dataset.h5\"], drop_columns=drop_columns)\n",
    "\n",
    "  # Remap condition indices and trial types (to match conventions of ham4_learn_4)\n",
    "  condition_idx_remap = {\n",
    "    1: 2, \n",
    "    2: 4,\n",
    "  }\n",
    "  df_ham4_learn_5[\"condition_idx\"] = df_ham4_learn_5[\"condition_idx\"].map(condition_idx_remap).fillna(df_ham4_learn_5[\"condition_idx\"])\n",
    "  df_ham4_learn_5[\"participant\"] = df_ham4_learn_5[\"participant\"] + df_ham4_learn_4[\"participant\"].max() + 1\n",
    "  df_ham4_learn_5[\"experiment_id\"] = \"ham4_learn_5\"\n",
    "  df_ham4_learn_5[\"data_round\"] = 5\n",
    "\n",
    "  df_ham4 = pd.concat([df_ham4_learn_4, df_ham4_learn_5])\n",
    "\n",
    "  df_ham4['perf'] = df_ham4['perf'].fillna(0)\n",
    "\n",
    "  df_ham4 = df_ham4.reset_index(drop=True)\n",
    "\n",
    "\n",
    "  ## Impute 4 missing condition_idx and trial_type values\n",
    "  mode_values = df_ham4.groupby('trialset_id').agg({\n",
    "      'condition_idx': lambda x: x.mode().iloc[0] if not x.mode().empty else None,\n",
    "      'trial_type': lambda x: x.mode().iloc[0] if not x.mode().empty else None\n",
    "  })\n",
    "  mode_values.columns = ['condition_idx_mode', 'trial_type_mode']\n",
    "\n",
    "  df_ham4 = df_ham4.merge(mode_values, on='trialset_id', how='left')\n",
    "\n",
    "  df_ham4['condition_idx'] = df_ham4['condition_idx'].fillna(df_ham4['condition_idx_mode'])\n",
    "  df_ham4['trial_type'] = df_ham4['trial_type'].fillna(df_ham4['trial_type_mode'])\n",
    "\n",
    "  df_ham4 = df_ham4.drop(['condition_idx_mode', 'trial_type_mode'], axis=1)\n",
    "\n",
    "  assert_constant_counts(df_ham4)\n",
    "\n",
    "  df_ham4 = reassign_blocks(df_ham4, verbose=False)\n",
    "\n",
    "  if DEIDENTIFIED_DATA:\n",
    "    print(\"Saving de-identified version of the dataset\")\n",
    "    df_ham4.to_csv(\"psych_data/df_ham4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ham4.groupby('condition_idx')['participant'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILTER OUT GUESSING PARTICIPANTS IN HAM4\n",
    "\n",
    "df_ham4_trials = df_ham4[df_ham4[\"trial_type\"].isin(trial_type_names_ham4)]\n",
    "\n",
    "df_ham4_calib = df_ham4[df_ham4[\"stimulus_name\"].isin([\"circle\", \"triangle\"])]\n",
    "\n",
    "calib_means = df_ham4_calib.groupby(\"participant\")[\"perf\"].mean()\n",
    "\n",
    "# Filter out participants with a mean calibration 'perf' of less than 0.9\n",
    "participants_calib_above09 = calib_means[calib_means >= 0.9].index\n",
    "\n",
    "print(f\"All participants: {df_ham4['participant'].nunique()}\")\n",
    "\n",
    "# Filter the DataFrame for these participants\n",
    "df_ham4 = df_ham4[df_ham4['participant'].isin(participants_calib_above09)]\n",
    "\n",
    "print(f\"Participants with calib acc of 0.9 and above: {df_ham4['participant'].nunique()}\")\n",
    "\n",
    "df_ham4_trials = df_ham4_trials[df_ham4_trials['participant'].isin(participants_calib_above09)]\n",
    "\n",
    "df_ham4_trials_test = df_ham4_trials[df_ham4_trials[\"block\"].isin([8, 9])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dermoscopy task learning performance statistics (Table 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_idx_ordering_ham4 = [0, 3, 4, 1, 2, 5]\n",
    "condition_labels_ham4 = [\"Control\", \"Enhance\", \"Enhance (shuffle)\", \"Select\", \"Select (shuffle)\", \"L-WISE\"]\n",
    "\n",
    "ham4_accuracy_df, ham4_training_time_df, ham4_completion_time_df = print_main_stats(df_ham4_trials, condition_idx_ordering_ham4, condition_labels_ham4, chance_level=0.25, test_blocks=[8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_square_comparisons(df_ham4_trials_test, condition_idx_ordering_ham4, condition_labels_ham4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_square_comparisons(df_ham4_trials_test, condition_idx_ordering_ham4, condition_labels_ham4, control_condition_idx=condition_labels_ham4.index(\"L-WISE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix Fig. S12E-G (learning curves for dermoscopy task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT HAM4 LEARNING CURVES\n",
    "\n",
    "control_condition = 0\n",
    "dirmap_ham4 = pd.read_csv('psych_data/dataset_dirmaps/ham4_dataset_dirmap.csv')\n",
    "create_comparison_plot(df_ham4_trials, dirmap_ham4, control_condition, trial_type_names_ham4.index(\"enhancement_taper_curriculum_sampling\"), trial_type_names_ham4, chance_level=0.25, ylim=[0.15, 0.85])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix Fig. S14B1: (relationship between human accuracy and ground truth logit in dermoscopy task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_logit_accuracy_relationship(df_ham4_trials, dirmap_ham4, test_blocks=[8,9], trial_type_names=trial_type_names_ham4, ylim=[0.1,0.75], control_only=True, bins=6, print_num_data_points=True, all_black=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix Fig. S14B2: (relationship between human accuracy and enhancement epsilon in dermoscopy task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_enhancement_accuracy_relationship(df_ham4_trials, dirmap_ham4, test_blocks=[0,1,2,3,4,5], trial_type_names=trial_type_names_ham4, condition_idx=4, min_acc=None, ylim=[0.3, 0.55])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix Fig. S16 (analyzing the effect of Greek name alias assignment on participant accuracy in the dermoscopy task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SUPPLEMENTARY CLASS MAPPING ANALYSIS PLOTS FOR HAM4\n",
    "\n",
    "classes = ['bcc', 'bkl', 'mel', 'nv']\n",
    "class_names = {\n",
    "    'mel': 'Melanoma',\n",
    "    'nv': 'Benign\\nMole',\n",
    "    'bcc': 'Basal Cell\\nCarcinoma',\n",
    "    'bkl': 'Benign\\nKeratosis'\n",
    "}\n",
    "class_mapping_plots(df_ham4_trials_test, classes, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MHIST histology learning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_type_names_mhist = [control_cond, \"enhancement_taper_curriculum_sampling\"]\n",
    "\n",
    "if os.path.isfile(\"psych_data/df_mhist.csv\") and DEIDENTIFIED_DATA:\n",
    "  print(\"Reading MHIST dataset from saved .csv\")\n",
    "  df_mhist = pd.read_csv(\"psych_data/df_mhist.csv\")\n",
    "else: # Load from .h5\n",
    "  print(\"Reading MHIST dataset from .h5 file\")\n",
    "\n",
    "  df_mhist = get_df_from_xarray([\"./results/mhist_learn_1_PARTIAL/mhist_learn_1_combined_dataset.h5\"], drop_columns=drop_columns)\n",
    "\n",
    "  df_mhist[\"experiment_id\"] = \"mhist_learn_1\"\n",
    "\n",
    "  df_mhist['perf'] = df_mhist['perf'].fillna(0)\n",
    "\n",
    "  df_mhist = df_mhist.reset_index(drop=True)\n",
    "\n",
    "  if DEIDENTIFIED_DATA:\n",
    "    print(\"Saving de-identified version of the dataset\")\n",
    "    df_mhist.to_csv(\"psych_data/df_mhist.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_mhist.groupby('condition_idx')['participant'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILTER OUT GUESSING PARTICIPANTS IN MHIST\n",
    "\n",
    "df_mhist_trials = df_mhist[df_mhist[\"trial_type\"].isin(trial_type_names_mhist)]\n",
    "\n",
    "df_mhist_calib = df_mhist[df_mhist[\"stimulus_name\"].isin([\"circle\", \"triangle\"])]\n",
    "\n",
    "calib_means = df_mhist_calib.groupby(\"participant\")[\"perf\"].mean()\n",
    "\n",
    "# Filter out participants with a mean calibration 'perf' of less than 0.9\n",
    "participants_calib_above09 = calib_means[calib_means >= 0.9].index\n",
    "\n",
    "print(f\"All participants: {df_mhist['participant'].nunique()}\")\n",
    "\n",
    "# Filter the DataFrame for these participants\n",
    "df_mhist = df_mhist[df_mhist['participant'].isin(participants_calib_above09)]\n",
    "\n",
    "print(f\"Participants with calib acc of 0.9 and above: {df_mhist['participant'].nunique()}\")\n",
    "\n",
    "df_mhist_trials = df_mhist_trials[df_mhist_trials['participant'].isin(participants_calib_above09)]\n",
    "\n",
    "df_mhist_trials_test = df_mhist_trials[df_mhist_trials[\"block\"].isin([8, 9])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histology task learning performance statistics (Table 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_idx_ordering_mhist = [0, 1]\n",
    "condition_labels_mhist = [\"Control\", \"L-WISE\"]\n",
    "\n",
    "mhist_accuracy_df, mhist_training_time_df, mhist_completion_time_df = print_main_stats(df_mhist_trials, condition_idx_ordering_mhist, condition_labels_mhist, chance_level=0.5, test_blocks=[8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_square_comparisons(df_mhist_trials_test, condition_idx_ordering_mhist, condition_labels_mhist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix Fig. S13D-F (learning curves for histology task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_condition = 0\n",
    "dirmap_mhist = pd.read_csv('psych_data/dataset_dirmaps/mhist_dataset_dirmap.csv')\n",
    "create_comparison_plot(df_mhist_trials, dirmap_mhist, control_condition, trial_type_names_mhist.index(\"enhancement_taper_curriculum_sampling\"), trial_type_names_mhist, chance_level=0.5, ylim=[0.4, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix Fig. S14C: (relationship between human accuracy and ground truth logit in histology task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_logit_accuracy_relationship(df_mhist_trials, dirmap_mhist, test_blocks=[8,9], trial_type_names=trial_type_names_mhist, ylim=[0.2,1.0], control_only=True, bins=6, print_num_data_points=True, all_black=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix Fig. S13C (Analysis of the relationship between ground truth logit and inter-annotator agreement in the MHIST histology dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new annotator_agreement column\n",
    "dirmap_mhist['annotator_agreement'] = abs(dirmap_mhist['Number of Annotators who Selected SSA (Out of 7)'] - 3.5) / 3.5\n",
    "\n",
    "# Calculate mean and confidence intervals for each unique annotator_agreement value\n",
    "unique_agreements = dirmap_mhist['annotator_agreement'].unique()\n",
    "means = []\n",
    "cis = []\n",
    "\n",
    "for agreement in unique_agreements:\n",
    "    data = dirmap_mhist[dirmap_mhist['annotator_agreement'] == agreement]['robust_gt_logit']\n",
    "    means.append(np.mean(data))\n",
    "    cis.append(bootstrap_ci(data))\n",
    "\n",
    "# Sort the data\n",
    "sorted_indices = np.argsort(unique_agreements)\n",
    "unique_agreements = unique_agreements[sorted_indices]\n",
    "means = np.array(means)[sorted_indices]\n",
    "cis = np.array(cis)[sorted_indices]\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(2, 4))\n",
    "\n",
    "# Plot horizontal bars\n",
    "bar_color = '#005885'  # Darker blue color\n",
    "ax.barh(unique_agreements, means, color=bar_color, edgecolor='black', linewidth=2, height=0.1)\n",
    "\n",
    "# Add error bars\n",
    "ax.errorbar(means, unique_agreements, xerr=np.abs(cis.T - means), fmt='none', ecolor='black', capsize=5, capthick=2, linewidth=2)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_ylabel('Annotator Agreement', fontsize=14)\n",
    "ax.set_xlabel(r'$L_{\\mathrm{gt}}$', fontsize=14)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "ax.set_yticks(unique_agreements, labels=['4', '5', '6', '7'])\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"notebooks/fig_outputs/mhist_agreement_Lgt.pdf\", dpi=300, format='pdf', bbox_inches='tight')\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = dirmap_mhist['robust_gt_logit'].corr(dirmap_mhist['annotator_agreement'])\n",
    "print(f\"Correlation between robust_gt_logit and annotator_agreement: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined figs with moth, dermoscopy, and histology tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig. 4A (Plot showing L-WISE improvements in test accuracy and training speed across 3 tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_acc_time_plot(idaea4_data, idaea4_training, ham4_data, ham4_training, mhist_data, mhist_training):\n",
    "    plt.style.use('seaborn-v0_8-white')\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    tasks = ['Natural images\\n(Moths)', 'Dermoscopy\\nimages', 'Histology\\nimages']\n",
    "    colors = ['orange', 'green', 'blue']\n",
    "\n",
    "    for i, (accuracy_data, training_data) in enumerate([(idaea4_data, idaea4_training), \n",
    "                                                        (ham4_data, ham4_training), \n",
    "                                                        (mhist_data, mhist_training)]):\n",
    "        for condition in ['Control', 'L-WISE']:\n",
    "            acc = accuracy_data[accuracy_data['Condition'] == condition]\n",
    "            train = training_data[training_data['condition'] == condition]\n",
    "            \n",
    "            xerr = [train['mean_training_time'] - train['ci_lower'], \n",
    "                    train['ci_upper'] - train['mean_training_time']]\n",
    "            yerr = [acc['yerr_lower'], acc['yerr_upper']]\n",
    "            \n",
    "            marker = 'o'\n",
    "            facecolor = 'none' if condition == 'Control' else colors[i]\n",
    "            label = tasks[i] if condition == 'Control' else None\n",
    "            \n",
    "            ax.errorbar(train['mean_training_time'], acc['Accuracy'], \n",
    "                        xerr=xerr, yerr=yerr, \n",
    "                        fmt=marker, color=colors[i], markerfacecolor=facecolor,\n",
    "                        capsize=5, label=label, markersize=10, \n",
    "                        linewidth=2, elinewidth=2, capthick=2)\n",
    "\n",
    "        # Add arrow\n",
    "        control = training_data[training_data['condition'] == 'Control']\n",
    "        lwise = training_data[training_data['condition'] == 'L-WISE']\n",
    "        control_acc = accuracy_data[accuracy_data['Condition'] == 'Control']\n",
    "        lwise_acc = accuracy_data[accuracy_data['Condition'] == 'L-WISE']\n",
    "        \n",
    "        # Calculate arrow start and end points with a gap\n",
    "        gap = 0.1  # Adjust this value to increase/decrease the gap\n",
    "        start_x = control['mean_training_time'].values[0]\n",
    "        start_y = control_acc['Accuracy'].values[0]\n",
    "        end_x = lwise['mean_training_time'].values[0]\n",
    "        end_y = lwise_acc['Accuracy'].values[0]\n",
    "        \n",
    "        dx = end_x - start_x\n",
    "        dy = end_y - start_y\n",
    "        arrow_length = np.sqrt(dx**2 + dy**2)\n",
    "        \n",
    "        start_x_adj = start_x + (dx * gap / arrow_length)\n",
    "        start_y_adj = start_y + (dy * gap / arrow_length)\n",
    "        end_x_adj = end_x - (dx * gap / arrow_length)\n",
    "        end_y_adj = end_y - (dy * gap / arrow_length)\n",
    "        \n",
    "        ax.annotate('', xy=(end_x_adj, end_y_adj), xytext=(start_x_adj, start_y_adj),\n",
    "                    arrowprops=dict(arrowstyle='->', color=colors[i], \n",
    "                                    linewidth=3, mutation_scale=20))\n",
    "\n",
    "    # Customize the plot\n",
    "    ax.set_xlabel('Training phase duration [min]', fontsize=24)\n",
    "    ax.set_ylabel('Test accuracy\\n[% correct]', fontsize=24)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=22)\n",
    "    \n",
    "    # Remove top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Add legend for tasks\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    legend1 = ax.legend(handles, labels, loc='upper right', fontsize=22, title_fontsize=22)\n",
    "    ax.add_artist(legend1)\n",
    "    \n",
    "    # Add a second legend for the markers\n",
    "    control_marker = plt.Line2D([0], [0], marker='o', color='k', markerfacecolor='none', markersize=10, label='\"Naive\" visual learning (control)')\n",
    "    lwise_marker = plt.Line2D([0], [0], marker='o', color='k', markerfacecolor='k', markersize=10, label='L-WISE (Ours)')\n",
    "    ax.legend(handles=[control_marker, lwise_marker], loc='lower left', fontsize=22, title_fontsize=22)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"notebooks/fig_outputs/combined_acc_time_plot.pdf\", dpi=300, format='pdf', bbox_inches='tight')\n",
    "\n",
    "create_combined_acc_time_plot(idaea4_accuracy_df, idaea4_training_time_df, ham4_accuracy_df, ham4_training_time_df, mhist_accuracy_df, mhist_training_time_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig. 4B (class-wise precision and recall for L-WISE participants vs controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_metrics(df_trials, class_name, condition_idx, trial_type_names, test_blocks=[8,9]):\n",
    "    \"\"\"Calculate precision and recall for a specific class and condition\"\"\"\n",
    "    # Filter for test blocks and condition\n",
    "    df_test = df_trials[\n",
    "        (df_trials['block'].isin(test_blocks)) & \n",
    "        (df_trials['condition_idx'] == condition_idx) & \n",
    "        (df_trials['trial_type'].isin(trial_type_names))\n",
    "    ]\n",
    "    \n",
    "    # Extract short class name\n",
    "    short_class = class_name.split('_')[-1]\n",
    "    \n",
    "    # Calculate participant-wise metrics\n",
    "    participant_metrics = []\n",
    "    for participant in df_test['participant'].unique():\n",
    "        df_participant = df_test[df_test['participant'] == participant]\n",
    "        \n",
    "        # True positives: correct predictions for this class\n",
    "        true_pos = df_participant[\n",
    "            (df_participant['class'] == class_name) & \n",
    "            (df_participant['perf'] == 1)\n",
    "        ].shape[0]\n",
    "        \n",
    "        # False positives: incorrect predictions of this class\n",
    "        false_pos = df_participant[\n",
    "            (df_participant['class'] != class_name) & \n",
    "            (~df_participant['i_choice'].isna()) &\n",
    "            (df_participant['i_choice'] == df_participant[df_participant['class'] == class_name]['i_choice'].iloc[0])\n",
    "        ].shape[0]\n",
    "        \n",
    "        # False negatives: incorrect predictions for examples of this class\n",
    "        false_neg = df_participant[\n",
    "            (df_participant['class'] == class_name) & \n",
    "            (df_participant['perf'] == 0)\n",
    "        ].shape[0]\n",
    "        \n",
    "        # Calculate metrics (adding small epsilon to prevent division by zero)\n",
    "        precision = true_pos / (true_pos + false_pos + 1e-10)\n",
    "        recall = true_pos / (true_pos + false_neg + 1e-10)\n",
    "        \n",
    "        participant_metrics.append({\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame for easy metric extraction\n",
    "    metrics_df = pd.DataFrame(participant_metrics)\n",
    "    \n",
    "    # Calculate bootstrap CIs\n",
    "    precision_ci = bootstrap_ci(metrics_df['precision'].values)\n",
    "    recall_ci = bootstrap_ci(metrics_df['recall'].values)\n",
    "    \n",
    "    return {\n",
    "        'class': short_class,\n",
    "        'precision': metrics_df['precision'].mean(),\n",
    "        'precision_ci': precision_ci,\n",
    "        'recall': metrics_df['recall'].mean(),\n",
    "        'recall_ci': recall_ci\n",
    "    }\n",
    "\n",
    "def create_precision_recall_comparison_scatter(tasks, ylim=[0, 1], plot_confidence_intervals=True):\n",
    "    \"\"\"\n",
    "    Create a scatter plot comparing Control vs L-WISE performance across tasks.\n",
    "    \n",
    "    Parameters:\n",
    "    tasks (list): List of dictionaries containing:\n",
    "        - df: DataFrame with trial data\n",
    "        - name: Display name for the task\n",
    "        - color: Color for plotting\n",
    "        - class_labels: Dictionary mapping class names to display labels\n",
    "        - trial_type_names: List of trial type names\n",
    "        - control_condition: Index for control condition\n",
    "        - lwise_condition: Index for L-WISE condition\n",
    "    ylim (list): Y-axis limits, should match X-axis limits\n",
    "    plot_confidence_intervals (bool): Whether to plot error bars\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (figure, axis)\n",
    "    \"\"\"\n",
    "    # Set up the style\n",
    "    plt.style.use('seaborn-v0_8-white')\n",
    "    plt.rcParams['xtick.bottom'] = True\n",
    "    plt.rcParams['ytick.left'] = True\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(7.5, 5))\n",
    "    \n",
    "    # Create axes with specific position to maintain square aspect\n",
    "    # [left, bottom, width, height]\n",
    "    ax = fig.add_axes([0.15, 0.15, 0.6, 0.75])\n",
    "    \n",
    "    # Plot diagonal line for equal performance\n",
    "    ax.plot([ylim[0], ylim[1]], [ylim[0], ylim[1]], '--', color='gray', linewidth=1.5)\n",
    "    \n",
    "    # Initialize handles for legends\n",
    "    task_handles = []\n",
    "    metric_handles = []\n",
    "    \n",
    "    # Plot each task\n",
    "    for task in tasks:\n",
    "        df = task['df']\n",
    "        color = task['color']\n",
    "        class_labels = task['class_labels']\n",
    "        trial_type_names = task['trial_type_names']\n",
    "        control_condition = task['control_condition']\n",
    "        lwise_condition = task['lwise_condition']\n",
    "        \n",
    "        # Get unique classes\n",
    "        classes = df['class'].unique()\n",
    "        \n",
    "        for class_name in classes:\n",
    "            # Calculate metrics\n",
    "            control_metrics = calculate_class_metrics(df, class_name, control_condition, trial_type_names)\n",
    "            lwise_metrics = calculate_class_metrics(df, class_name, lwise_condition, trial_type_names)\n",
    "            \n",
    "            # Get display label\n",
    "            display_label = class_labels[control_metrics['class']]\n",
    "            \n",
    "            # Plot precision (triangles)\n",
    "            if plot_confidence_intervals:\n",
    "                ax.errorbar(control_metrics['precision'], lwise_metrics['precision'],\n",
    "                           xerr=[[control_metrics['precision'] - control_metrics['precision_ci'][0]],\n",
    "                                [control_metrics['precision_ci'][1] - control_metrics['precision']]],\n",
    "                           yerr=[[lwise_metrics['precision'] - lwise_metrics['precision_ci'][0]],\n",
    "                                [lwise_metrics['precision_ci'][1] - lwise_metrics['precision']]],\n",
    "                           fmt='none', color='black', capsize=5, elinewidth=1, capthick=1)\n",
    "            prec = ax.scatter(control_metrics['precision'], lwise_metrics['precision'],\n",
    "                            marker='^', s=60, c=color, label=display_label,\n",
    "                            edgecolor='black', linewidth=1)\n",
    "            \n",
    "            # Plot recall (squares)\n",
    "            if plot_confidence_intervals:\n",
    "                ax.errorbar(control_metrics['recall'], lwise_metrics['recall'],\n",
    "                           xerr=[[control_metrics['recall'] - control_metrics['recall_ci'][0]],\n",
    "                                [control_metrics['recall_ci'][1] - control_metrics['recall']]],\n",
    "                           yerr=[[lwise_metrics['recall'] - lwise_metrics['recall_ci'][0]],\n",
    "                                [lwise_metrics['recall_ci'][1] - lwise_metrics['recall']]],\n",
    "                           fmt='none', color='black', capsize=5, elinewidth=1, capthick=1)\n",
    "            rec = ax.scatter(control_metrics['recall'], lwise_metrics['recall'],\n",
    "                           marker='s', s=60, c=color, label=display_label,\n",
    "                           edgecolor='black', linewidth=1)\n",
    "        \n",
    "        # Add to task handles (only once per task)\n",
    "        task_handles.append(plt.scatter([], [], c=color, label=task['name'],\n",
    "                                      marker='o', s=60, edgecolor='black', linewidth=1))\n",
    "    \n",
    "    # Add metric handles\n",
    "    metric_handles.extend([\n",
    "        plt.scatter([], [], c='gray', marker='^', label='Precision',\n",
    "                   s=60, edgecolor='black', linewidth=1),\n",
    "        plt.scatter([], [], c='gray', marker='s', label='Recall',\n",
    "                   s=60, edgecolor='black', linewidth=1)\n",
    "    ])\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_xlabel('Control', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('L-WISE', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Set equal aspect ratio and limits\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim(ylim)\n",
    "    ax.set_ylim(ylim)\n",
    "    \n",
    "    # Style spines and ticks\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_linewidth(2)\n",
    "    ax.spines['bottom'].set_linewidth(2)\n",
    "    ax.tick_params(axis='both', which='major', length=6, width=2, labelsize=12)\n",
    "    ax.tick_params(axis='both', which='minor', length=3, width=1)\n",
    "    \n",
    "    # Create separate legends with adjusted positioning and smaller font\n",
    "    task_legend = ax.legend(handles=task_handles, title='Tasks',\n",
    "                           bbox_to_anchor=(1.05, 1), loc='upper left',\n",
    "                           frameon=False, fontsize=10, title_fontsize=11)\n",
    "    ax.add_artist(task_legend)\n",
    "    \n",
    "    metric_legend = ax.legend(handles=metric_handles, title='Metrics',\n",
    "                            bbox_to_anchor=(1.05, 0.3), loc='center left',\n",
    "                            frameon=False, fontsize=10, title_fontsize=11)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(\"notebooks/fig_outputs/precision_recall_scatter.pdf\", dpi=300, format='pdf', bbox_inches='tight')\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idaea4_class_label_map = {\n",
    "  \"seriata\": \"$\\\\it{seriata}$\",\n",
    "  \"tacturata\": \"$\\\\it{tacturata}$\",\n",
    "  \"biselata\": \"$\\\\it{biselata}$\",\n",
    "  \"aversata\": \"$\\\\it{aversata}$\"\n",
    "}\n",
    "\n",
    "ham4_class_label_map = {\n",
    "  \"nv\": \"Benign mole\",\n",
    "  \"mel\": \"Melanoma\",\n",
    "  \"bcc\": \"Basal cell carcinoma\",\n",
    "  \"bkl\": \"Benign keratosis\"\n",
    "}\n",
    "\n",
    "mhist_class_label_map = {\n",
    "  \"ssa\": \"Sessile serrated adenoma (malignant)\",\n",
    "  \"hp\": \"Hyperplastic polyp (benign)\",\n",
    "}\n",
    "\n",
    "tasks = [\n",
    "    {\n",
    "        'df': df_idaea4_trials,\n",
    "        'name': 'Moth photographs',\n",
    "        'color': 'orange',\n",
    "        'class_labels': idaea4_class_label_map,\n",
    "        'trial_type_names': trial_type_names_idaea4,\n",
    "        'control_condition': 0,\n",
    "        'lwise_condition': trial_type_names_idaea4.index(\"enhancement_taper_curriculum_sampling\")\n",
    "    },\n",
    "    {\n",
    "        'df': df_ham4_trials,\n",
    "        'name': 'Dermoscopy images',\n",
    "        'color': 'green',\n",
    "        'class_labels': ham4_class_label_map,\n",
    "        'trial_type_names': trial_type_names_ham4,\n",
    "        'control_condition': 0,\n",
    "        'lwise_condition': trial_type_names_ham4.index(\"enhancement_taper_curriculum_sampling\")\n",
    "    },\n",
    "    {\n",
    "        'df': df_mhist_trials,\n",
    "        'name': 'Histology images',\n",
    "        'color': 'blue',\n",
    "        'class_labels': mhist_class_label_map,\n",
    "        'trial_type_names': trial_type_names_mhist,\n",
    "        'control_condition': 0,\n",
    "        'lwise_condition': 1  # Assuming this is the correct index for MHIST\n",
    "    }\n",
    "]\n",
    "\n",
    "fig, ax = create_precision_recall_comparison_scatter(tasks, ylim=[0.2, 0.8], plot_confidence_intervals=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix Fig. S12L-M: HAM4 dermoscopy pilot study (initial enhancement epsilon was too high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_cond = \"natural\"\n",
    "non_natural_cond = \"enhanced\"\n",
    "trial_type_names_ham4_pilot = [control_cond, non_natural_cond]\n",
    "\n",
    "if os.path.isfile(\"psych_data/df_ham4_pilot.csv\") and DEIDENTIFIED_DATA:\n",
    "  print(\"Reading ham4 pilot dataset from saved .csv\")\n",
    "  df_ham4_pilot = pd.read_csv(\"psych_data/df_ham4_pilot.csv\")\n",
    "else: # Load from .h5\n",
    "  print(\"Reading ham4 pilot dataset from .h5 file\")\n",
    "\n",
    "  data_paths = [ # ham4_learn_0\n",
    "    \"./results/combined_dataset_37YYO3NWHDBLNHVEBAN4G8XJE1YCCC.h5\",\n",
    "    \"./results/combined_dataset_31MCUE39BK7ARTF0K38MDWE46HH3GL.h5\",\n",
    "  ]\n",
    "\n",
    "  df_ham4_pilot = get_df_from_xarray(data_paths, drop_columns=drop_columns)\n",
    "\n",
    "  df_ham4_pilot[\"experiment_id\"] = \"ham4-pilot_learn_0\"\n",
    "\n",
    "  df_ham4_pilot['perf'] = df_ham4_pilot['perf'].fillna(0)\n",
    "\n",
    "  if DEIDENTIFIED_DATA:\n",
    "    print(\"Saving de-identified version of the dataset\")\n",
    "    df_ham4_pilot.to_csv(\"psych_data/df_ham4_pilot.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FILTER OUT GUESSING PARTICIPANTS IN HAM4\n",
    "\n",
    "df_ham4_pilot_trials = df_ham4_pilot[df_ham4_pilot[\"trial_type\"].isin(trial_type_names_ham4_pilot)]\n",
    "\n",
    "df_ham4_pilot_calib = df_ham4_pilot[df_ham4_pilot[\"stimulus_name\"].isin([\"circle\", \"triangle\"])]\n",
    "\n",
    "calib_means = df_ham4_pilot_calib.groupby(\"participant\")[\"perf\"].mean()\n",
    "\n",
    "# Filter out participants with a mean calibration 'perf' of less than 0.9\n",
    "participants_calib_above09 = calib_means[calib_means >= 0.9].index\n",
    "\n",
    "print(f\"All participants: {df_ham4_pilot['participant'].nunique()}\")\n",
    "\n",
    "# Filter the DataFrame for these participants\n",
    "df_ham4_pilot = df_ham4_pilot[df_ham4_pilot['participant'].isin(participants_calib_above09)]\n",
    "\n",
    "print(f\"Participants with calib acc of 0.9 and above: {df_ham4_pilot['participant'].nunique()}\")\n",
    "\n",
    "df_ham4_pilot_trials = df_ham4_pilot_trials[df_ham4_pilot_trials['participant'].isin(participants_calib_above09)]\n",
    "\n",
    "df_ham4_pilot_trials_test = df_ham4_pilot_trials[df_ham4_pilot_trials[\"block\"].isin([6, 7])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_idx_ordering = [0, 1]\n",
    "condition_labels = [\"Control\", \"Enhance\"]\n",
    "\n",
    "print_main_stats(df_ham4_pilot_trials, condition_idx_ordering, condition_labels, chance_level=0.25, test_blocks=[6,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_square_comparisons(df_ham4_pilot_trials_test, condition_idx_ordering, condition_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT HAM4 PILOT LEARNING CURVES\n",
    "\n",
    "control_condition = 0\n",
    "create_comparison_plot(df_ham4_pilot_trials, dirmap_ham4, control_condition, trial_type_names_ham4_pilot.index(\"enhanced\"), trial_type_names_ham4_pilot, chance_level=0.25, ylim=[0.15, 0.85], max_enhance_eps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageNet 16-way animal classification experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imagenet16_dataset(data_path, RUN_TESTS=True, drop_columns=None):\n",
    "  # Load dataset and convert to a dataframe with 1 row per trial\n",
    "  ds = xr.open_dataset(data_path)\n",
    "\n",
    "  raw_df = ds.to_dataframe().reset_index()\n",
    "\n",
    "  # Filter rows where choice_slot equals i_choice\n",
    "  df = raw_df[raw_df['choice_slot'] == raw_df['i_choice']].copy()\n",
    "\n",
    "  # Sort dataframe such that trials for each participant appear in order\n",
    "  df = df.sort_values(by=['participant', 'obs'])\n",
    "\n",
    "  # Sort the columns in a logical order\n",
    "  ordered_cols = ['participant', 'condition_idx', 'block', 'obs', 'trial_type', 'class', 'stimulus_image_url', 'stimulus_name', 'choice_name', 'i_correct_choice', 'i_choice', 'perf', 'reaction_time_msec', 'rel_timestamp_response', 'timestamp_start', 'monitor_width_px', 'monitor_height_px', 'stimulus_width_px', 'choice_width_px', 'stimulus_duration_msec', 'post_stimulus_delay_duration_msec', 'pre_choice_lockout_delay_duration_msec']\n",
    "  other_cols = [col for col in df.columns if col not in ordered_cols]\n",
    "  df = df[ordered_cols + other_cols]\n",
    "\n",
    "  # Recover info about whether each image was from train, val, etc.\n",
    "  df[\"split\"] = df.apply(lambda row: row['stimulus_image_url'].split(\".s3.amazonaws.com/\")[1].split(\"/\")[0], axis=1)\n",
    "\n",
    "  if RUN_TESTS:\n",
    "    # Sanity check that will almost certainly fail if stimulus_name and choice_name are calculated incorrectly\n",
    "    for _, row in df.iterrows():\n",
    "      if row[\"i_choice\"] == row[\"i_correct_choice\"]:\n",
    "        assert(row[\"stimulus_name\"] == row[\"choice_name\"]), \"stim=\" + row[\"stimulus_name\"] + \", choice=\" + row[\"choice_name\"]\n",
    "      else:\n",
    "        assert(row[\"stimulus_name\"] != row[\"choice_name\"]), \"stim=\" + row[\"stimulus_name\"] + \", choice=\" + row[\"choice_name\"]\n",
    "\n",
    "  df.to_csv(data_path.replace(\".h5\", \".csv\"))\n",
    "\n",
    "  if drop_columns is not None:\n",
    "    df = df.drop(drop_columns, axis=1, errors='ignore')\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEIDENTIFIED_DATA and os.path.isfile(\"psych_data/df_main_i16.csv\") and os.path.isfile(\"psych_data/df_backbone_compare_i16.csv\") and os.path.isfile(\"psych_data/df_loss_ablation_i16.csv\"):\n",
    "    print(\"Reading imagenet datasets from saved .csv\")\n",
    "    df_main_i16 = pd.read_csv(\"psych_data/df_main_i16.csv\")\n",
    "    df_backbone_compare_i16 = pd.read_csv(\"psych_data/df_backbone_compare_i16.csv\")\n",
    "    df_loss_ablation_i16 = pd.read_csv(\"psych_data/df_loss_ablation_i16.csv\")\n",
    "else: # Load from raw .h5\n",
    "    print(\"Reading imagenet datasets from .h5 files\")\n",
    "\n",
    "    df_main_i16 = load_imagenet16_dataset(\"./results/imagenet16_v1_mod_2/imagenet16_v1_mod_2_combined_dataset.h5\", drop_columns=drop_columns)\n",
    "    df_main_i16[\"experiment_id\"] = \"imagenet16_main\"\n",
    "\n",
    "    df_backbone_compare_i16 = load_imagenet16_dataset(\"./results/imagenet16_v1_mod_4/imagenet16_v1_mod_4_combined_dataset.h5\", drop_columns=drop_columns)\n",
    "    df_backbone_compare_i16['participant'] += int(df_main_i16[\"participant\"].max()+1)\n",
    "    df_backbone_compare_i16[\"experiment_id\"] = \"imagenet16_backbone_compare\"\n",
    "\n",
    "    df_loss_ablation_i16 = load_imagenet16_dataset(\"./results/imagenet16_v1_mod_1/imagenet16_v1_mod_1_combined_dataset.h5\", drop_columns=drop_columns)\n",
    "    df_loss_ablation_i16['participant'] += int(df_backbone_compare_i16[\"participant\"].max()+1)\n",
    "    df_loss_ablation_i16[\"experiment_id\"] = \"imagenet16_loss_ablation\"\n",
    "\n",
    "    # Trial type remapping for backbone comparison experiment (forgot to change trial type names in config)\n",
    "    trial_type_remap = {\n",
    "        # Erroneous -> Correct mappings\n",
    "        \"natural\": \"natural\",  # already correct\n",
    "        \"enhanced_logit_5\": \"enhanced_vanilla_resnet50\",\n",
    "        \"enhanced_logit_10\": \"enhanced_eps1_resnet50\",\n",
    "        \"enhanced_logit_15\": \"enhanced_eps3_resnet50\",\n",
    "        \"enhanced_logit_20\": \"enhanced_eps10_resnet50\",\n",
    "        \"attacked_logit_10\": \"enhanced_cutmix_resnet50\",\n",
    "        \"enhanced_auto_lr\": \"enhanced_xcit_augmented\",\n",
    "        \"enhanced_clahe_2\": \"enhanced_vit_harmonized_augmented\",\n",
    "        \"enhanced_msrcr\": \"enhanced_eps3_resnet50_augmented\",\n",
    "        \n",
    "        # Correct -> Correct mappings (in case they appear)\n",
    "        \"enhanced_vanilla_resnet50\": \"enhanced_vanilla_resnet50\",\n",
    "        \"enhanced_eps1_resnet50\": \"enhanced_eps1_resnet50\",\n",
    "        \"enhanced_eps3_resnet50\": \"enhanced_eps3_resnet50\",\n",
    "        \"enhanced_eps10_resnet50\": \"enhanced_eps10_resnet50\",\n",
    "        \"enhanced_cutmix_resnet50\": \"enhanced_cutmix_resnet50\",\n",
    "        \"enhanced_xcit_augmented\": \"enhanced_xcit_augmented\",\n",
    "        \"enhanced_vit_harmonized_augmented\": \"enhanced_vit_harmonized_augmented\",\n",
    "        \"enhanced_eps3_resnet50_augmented\": \"enhanced_eps3_resnet50_augmented\"\n",
    "    }\n",
    "\n",
    "    # Map the trial types using the dictionary, keeping original values if not in mapping\n",
    "    df_backbone_compare_i16['trial_type'] = df_backbone_compare_i16['trial_type'].map(lambda x: trial_type_remap.get(x, x))\n",
    "\n",
    "    # Remove irrelevant/unused trial types\n",
    "    df_main_i16 = df_main_i16[df_main_i16[\"trial_type\"] != \"attacked_logit_10\"]\n",
    "    df_backbone_compare_i16 = df_backbone_compare_i16[df_backbone_compare_i16[\"trial_type\"] != \"enhanced_eps3_resnet50_augmented\"]\n",
    "    df_backbone_compare_i16 = df_backbone_compare_i16[df_backbone_compare_i16[\"trial_type\"] != \"enhanced_vit_harmonized_augmented\"]\n",
    "\n",
    "    if DEIDENTIFIED_DATA:\n",
    "        print(\"Saving de-identified version of the dataset\")\n",
    "        df_main_i16.to_csv(\"psych_data/df_main_i16.csv\", index=False)\n",
    "        df_backbone_compare_i16.to_csv(\"psych_data/df_backbone_compare_i16.csv\", index=False)\n",
    "        df_loss_ablation_i16.to_csv(\"psych_data/df_loss_ablation_i16.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lwise_psych_modules.generate_trials_helpers import replace_bucket_name_in_url\n",
    "\n",
    "# Filter out participants with calibration accuracy below 0.9\n",
    "\n",
    "def filter_calib_below_09(df, df_name):\n",
    "  pt_count_init = df['participant'].nunique()\n",
    "  df_calib = df[df[\"trial_type\"] == \"calibration\"]\n",
    "  calib_means = df_calib.groupby(\"participant\")[\"perf\"].mean()\n",
    "  participants_calib_above09 = calib_means[calib_means >= 0.9].index\n",
    "  df = df[df['participant'].isin(participants_calib_above09)]\n",
    "  df = df[df[\"trial_type\"] != \"calibration\"]\n",
    "  print(f\"In {df_name}, {df['participant'].nunique()} out of {pt_count_init} participants had calibration accuracy of 0.9 or above\")\n",
    "  return df\n",
    "\n",
    "df_main_i16 = filter_calib_below_09(df_main_i16, \"df_main_i16\")\n",
    "df_backbone_compare_i16 = filter_calib_below_09(df_backbone_compare_i16, \"df_backbone_compare_i16\")\n",
    "df_loss_ablation_i16 = filter_calib_below_09(df_loss_ablation_i16, \"df_loss_ablation_i16\")\n",
    "\n",
    "# Remove screening and warmup trials\n",
    "df_main_i16 = df_main_i16[df_main_i16[\"block\"] > 1]\n",
    "df_backbone_compare_i16 = df_backbone_compare_i16[df_backbone_compare_i16[\"block\"] > 1]\n",
    "df_loss_ablation_i16 = df_loss_ablation_i16[df_loss_ablation_i16[\"block\"] > 1]\n",
    "\n",
    "# Retrieve ground truth logit values from ImageNet_eps3.pt model\n",
    "\n",
    "def find_value_by_url(url, df, col_name):\n",
    "  filtered_df = df[df['url'] == url]\n",
    "  if len(filtered_df) > 1:\n",
    "    raise ValueError(\"More than one row found for the given URL.\")\n",
    "  elif len(filtered_df) == 0:\n",
    "    return None\n",
    "  else:\n",
    "    return filtered_df.iloc[0][col_name]\n",
    "\n",
    "def get_gt_logit_of_image_version(trial_df, logit_df):\n",
    "  def get_logit_from_row(row, logit_df):\n",
    "    bucket_name = row[\"stimulus_image_url\"].split(\"://\")[1].split(\".s3.amazonaws.com\")[0]\n",
    "    robust_logit_col_name_prefix = bucket_name.replace(\"dot\", \".\").replace(\"-\", \"_\").replace(\"morgan_\", \"\").replace(\"imagenet16_\", \"\").replace(\"imagenet16\", \"\")\n",
    "    if len(robust_logit_col_name_prefix) > 0: \n",
    "      robust_logit_col_name = robust_logit_col_name_prefix + \"_robust_gt_logit\"\n",
    "    else:\n",
    "      robust_logit_col_name = \"robust_gt_logit\"\n",
    "\n",
    "    return find_value_by_url(replace_bucket_name_in_url(row[\"stimulus_image_url\"].split(\"?\")[0], \"morgan-imagenet16\"), logit_df, robust_logit_col_name)\n",
    "\n",
    "  trial_df[\"robust_gt_logit\"] = trial_df.apply(lambda row: get_logit_from_row(row, logit_df), axis=1)\n",
    "  return trial_df\n",
    "\n",
    "logit_df = pd.read_csv(\"psych_data/dataset_dirmaps/imagenet_animals_dataset_dirmap.csv\")\n",
    "\n",
    "df_main_i16 = get_gt_logit_of_image_version(df_main_i16, logit_df)\n",
    "df_backbone_compare_i16 = get_gt_logit_of_image_version(df_backbone_compare_i16, logit_df)\n",
    "df_loss_ablation_i16 = get_gt_logit_of_image_version(df_loss_ablation_i16, logit_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig. 1B1 (effect of image enhancement on human accuracy in ImageNet animal task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZE = False\n",
    "\n",
    "# First, calculate the mean 'perf' for each participant on 'natural' trials\n",
    "natural_means = df_main_i16[df_main_i16['trial_type'] == 'natural'].groupby('participant')['perf'].mean()\n",
    "\n",
    "# Create a function to normalize performance\n",
    "def normalize_performance(row):\n",
    "    return row['perf'] / natural_means[row['participant']]\n",
    "\n",
    "# Apply the normalization to all rows\n",
    "df_main_i16['normalized_perf'] = df_main_i16.apply(normalize_performance, axis=1)\n",
    "perf_metric = 'normalized_perf' if NORMALIZE else 'perf'\n",
    "\n",
    "# Group by participant and trial_type, then calculate mean accuracy\n",
    "accuracy_by_group = df_main_i16[df_main_i16['trial_type'].isin(['natural', 'enhanced_logit_5', 'enhanced_logit_10', 'enhanced_logit_15', 'enhanced_logit_20', 'enhanced_clahe_2', 'enhanced_msrcr', 'enhanced_auto_lr'])].groupby(['participant', 'trial_type'])[perf_metric].mean().reset_index()\n",
    "\n",
    "# Function to calculate bootstrap confidence interval\n",
    "def bootstrap_ci(data, num_bootstrap_samples=10000, ci=0.95):\n",
    "    bootstrap_means = np.random.choice(data, (num_bootstrap_samples, len(data)), replace=True).mean(axis=1)\n",
    "    return np.percentile(bootstrap_means, [(1 - ci) / 2 * 100, (1 + ci) / 2 * 100])\n",
    "\n",
    "# Calculate overall mean accuracy and bootstrap CI for each group\n",
    "mean_accuracy_by_group = accuracy_by_group.groupby('trial_type')[perf_metric].mean().reset_index()\n",
    "ci_accuracy_by_group = accuracy_by_group.groupby('trial_type')[perf_metric].apply(bootstrap_ci).reset_index()\n",
    "ci_accuracy_by_group[['ci_lower', 'ci_upper']] = pd.DataFrame(ci_accuracy_by_group[perf_metric].tolist(), index=ci_accuracy_by_group.index)\n",
    "ci_accuracy_by_group = ci_accuracy_by_group.drop(perf_metric, axis=1)\n",
    "\n",
    "# Merge mean and CI dataframes\n",
    "result = pd.merge(mean_accuracy_by_group, ci_accuracy_by_group, on='trial_type')\n",
    "\n",
    "# Define the order of trial types and their labels\n",
    "trial_types = ['natural', 'enhanced_logit_5', 'enhanced_logit_10', 'enhanced_logit_15', 'enhanced_logit_20', 'enhanced_clahe_2', 'enhanced_msrcr', 'enhanced_auto_lr']\n",
    "labels = ['ϵ = 0', 'ϵ = 5', 'ϵ = 10', 'ϵ = 15', 'ϵ = 20', 'CLAHE', 'MSRCR', 'LR']\n",
    "\n",
    "# Set the style for a more professional look\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"deep\")\n",
    "\n",
    "# Increase the figure size and DPI for better quality, but make it more compact\n",
    "fig, ax = plt.subplots(figsize=(7, 3), dpi=300)\n",
    "\n",
    "# Prepare data for plotting\n",
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7])  # Adjust x-coordinates\n",
    "means = result.set_index('trial_type').loc[trial_types, perf_metric]\n",
    "ci_lower = result.set_index('trial_type').loc[trial_types, 'ci_lower']\n",
    "ci_upper = result.set_index('trial_type').loc[trial_types, 'ci_upper']\n",
    "yerr = np.array([means - ci_lower, ci_upper - means])\n",
    "\n",
    "# Plot points with error bars\n",
    "ax.errorbar(x, means, yerr=yerr, fmt='o', capsize=5, capthick=2, elinewidth=2, markersize=8)\n",
    "\n",
    "# Connect the first 5 points with a line\n",
    "ax.plot(x[:5], means[:5], '-', linewidth=2)\n",
    "\n",
    "# Add a red horizontal dotted line at the level of the ϵ = 0 point\n",
    "ax.axhline(y=means[0], color='red', linestyle=':', linewidth=2)\n",
    "\n",
    "# Customize the plot\n",
    "ylabel = 'Normalized Mean Accuracy' if NORMALIZE else 'Mean Accuracy'\n",
    "ax.set_ylabel(ylabel, fontsize=12, fontweight='bold')\n",
    "#ax.set_xlabel('Condition', fontsize=16, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, fontsize=12, fontweight='bold')\n",
    "\n",
    "# Increase tick label size\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "# Set y-axis limits\n",
    "ax.set_ylim(0.675 if not NORMALIZE else 0.8, 0.875 if not NORMALIZE else 1.2)\n",
    "\n",
    "# Add subtle spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(1.5)\n",
    "ax.spines['bottom'].set_linewidth(1.5)\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "# Optionally, save the figure as a high-resolution image\n",
    "plt.savefig('notebooks/fig_outputs/imagenet16_accuracy_plot.pdf', dpi=2400, format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig. 1A1 (relationship between robust ground truth logit and human accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main_i16_nat = df_main_i16[df_main_i16[\"trial_type\"].isin(['natural', 'enhanced_clahe_2', 'enhanced_msrcr', 'enhanced_auto_lr'])]\n",
    "df_backbone_compare_i16_nat = df_backbone_compare_i16[df_backbone_compare_i16[\"trial_type\"].isin(['natural', 'enhanced_vanilla_resnet50', 'enhanced_cutmix_resnet50'])]\n",
    "df_loss_ablation_i16_nat = df_loss_ablation_i16[(df_loss_ablation_i16[\"trial_type\"] == 'natural') & (df_loss_ablation_i16[\"split\"] == \"val\")]\n",
    "\n",
    "df_combined_i16_nat = pd.concat([df_main_i16_nat, df_backbone_compare_i16_nat, df_loss_ablation_i16_nat])\n",
    "print(\"Number of trials for plot:\", len(df_combined_i16_nat))\n",
    "print(\"Number of participants for plot:\", df_combined_i16_nat[\"participant\"].nunique())\n",
    "\n",
    "analyze_logit_accuracy_relationship(df_combined_i16_nat, None, test_blocks=None, trial_type_names=None, ylim=[0.2,1], xlim=[-3,28], control_only=False, bins=10, print_num_data_points=False, retrieve_logits=False, fig_size=(3.5, 3), all_black=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix Fig. S5 (relationship between robust ground truth logit and human accuracy, strictly including only original, unmodified images (none from Retinex, vanilla models, etc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main_i16_nat = df_main_i16[df_main_i16[\"trial_type\"].isin(['natural'])]\n",
    "df_backbone_compare_i16_nat = df_backbone_compare_i16[df_backbone_compare_i16[\"trial_type\"].isin(['natural'])]\n",
    "df_loss_ablation_i16_nat = df_loss_ablation_i16[(df_loss_ablation_i16[\"trial_type\"] == 'natural') & (df_loss_ablation_i16[\"split\"] == \"val\")]\n",
    "\n",
    "df_combined_i16_nat = pd.concat([df_main_i16_nat, df_backbone_compare_i16_nat, df_loss_ablation_i16_nat]) # This one looks worse because less data\n",
    "print(\"Number of trials for plot:\", len(df_combined_i16_nat))\n",
    "print(\"Number of participants for plot:\", df_combined_i16_nat[\"participant\"].nunique())\n",
    "\n",
    "analyze_logit_accuracy_relationship(df_combined_i16_nat, None, test_blocks=None, trial_type_names=None, ylim=[0.2,1], xlim=[-3,28], control_only=False, bins=10, print_num_data_points=False, retrieve_logits=False, fig_size=(3.5, 3), fig_file_name=\"imagenet16_robust_gt_logit_accuracy_strict_natural.pdf\", all_black=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix Fig. S6 (comparison of image difficulty prediction methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "df_combined_i16_nat_strict = df_combined_i16_nat[df_combined_i16_nat[\"trial_type\"] == \"natural\"]\n",
    "\n",
    "df_combined_i16_nat_strict[\"vanilla_gt_logit\"] = df_combined_i16_nat_strict.apply(lambda row: find_value_by_url(replace_bucket_name_in_url(row[\"stimulus_image_url\"].split(\"?\")[0], \"morgan-imagenet16\"), logit_df, \"vanilla_gt_logit\"), axis=1)\n",
    "\n",
    "with open(\"psych_data/imagenet_animals_image_difficulty_metrics/cscore_proxy_robust.json\", \"r\") as file:\n",
    "  cscore_robust = json.load(file)\n",
    "with open(\"psych_data/imagenet_animals_image_difficulty_metrics/cscore_proxy_vanilla.json\", \"r\") as file:\n",
    "  cscore_vanilla = json.load(file)\n",
    "\n",
    "with open(\"psych_data/imagenet_animals_image_difficulty_metrics/pred_depth_robust.json\", \"r\") as file:\n",
    "  pred_depth_robust = json.load(file)\n",
    "with open(\"psych_data/imagenet_animals_image_difficulty_metrics/pred_depth_vanilla.json\", \"r\") as file:\n",
    "  pred_depth_vanilla = json.load(file)\n",
    "\n",
    "with open(\"psych_data/imagenet_animals_image_difficulty_metrics/adv_eps_robust.json\", \"r\") as file:\n",
    "  adv_eps_robust = json.load(file)\n",
    "with open(\"psych_data/imagenet_animals_image_difficulty_metrics/adv_eps_vanilla.json\", \"r\") as file:\n",
    "  adv_eps_vanilla = json.load(file)\n",
    "\n",
    "df_combined_i16_nat_strict[\"cscore_robust\"] = df_combined_i16_nat_strict.apply(lambda row: cscore_robust[row[\"stimulus_image_url\"].split(\"?\")[0].split(\".com/\")[1]], axis=1)\n",
    "df_combined_i16_nat_strict[\"cscore_vanilla\"] = df_combined_i16_nat_strict.apply(lambda row: cscore_vanilla[row[\"stimulus_image_url\"].split(\"?\")[0].split(\".com/\")[1]], axis=1)\n",
    "df_combined_i16_nat_strict[\"pred_depth_robust\"] = df_combined_i16_nat_strict.apply(lambda row: pred_depth_robust[row[\"stimulus_image_url\"].split(\"?\")[0].split(\".com/\")[1]], axis=1)\n",
    "df_combined_i16_nat_strict[\"pred_depth_vanilla\"] = df_combined_i16_nat_strict.apply(lambda row: pred_depth_vanilla[row[\"stimulus_image_url\"].split(\"?\")[0].split(\".com/\")[1]], axis=1)\n",
    "df_combined_i16_nat_strict[\"adv_eps_robust\"] = df_combined_i16_nat_strict.apply(lambda row: adv_eps_robust[row[\"stimulus_image_url\"].split(\"?\")[0].split(\".com/\")[1]], axis=1)\n",
    "df_combined_i16_nat_strict[\"adv_eps_vanilla\"] = df_combined_i16_nat_strict.apply(lambda row: adv_eps_vanilla[row[\"stimulus_image_url\"].split(\"?\")[0].split(\".com/\")[1]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def bootstrap_ci(data, num_bootstrap_samples=10000, confidence_level=0.95):\n",
    "    bootstrap_means = np.array([np.mean(np.random.choice(data, size=len(data), replace=True)) \n",
    "                                for _ in range(num_bootstrap_samples)])\n",
    "    return np.percentile(bootstrap_means, [(1 - confidence_level) / 2 * 100, (1 + confidence_level) / 2 * 100])\n",
    "\n",
    "def perform_cv_analysis(X, y, feature_name):\n",
    "    # Remove any rows with NaN values\n",
    "    mask = ~np.isnan(X).any(axis=1)\n",
    "    X_clean = X[mask]\n",
    "    y_clean = y[mask]\n",
    "    \n",
    "    if len(X_clean) < len(X):\n",
    "        print(f\"{feature_name}: {len(X) - len(X_clean)} rows removed due to NaN values\")\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_clean)\n",
    "    \n",
    "    log_reg = LogisticRegression()\n",
    "    cv = StratifiedKFold(n_splits=500, shuffle=True, random_state=42)\n",
    "    auc_scores = cross_val_score(log_reg, X_scaled, y_clean, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    ci_lower, ci_upper = bootstrap_ci(auc_scores)\n",
    "    \n",
    "    return mean_auc, ci_lower, ci_upper, len(X_clean)\n",
    "\n",
    "def perform_statistical_analysis(X, y, feature_names):\n",
    "    # Remove any rows with NaN values\n",
    "    mask = ~np.isnan(X).any(axis=1)\n",
    "    X_clean = X[mask]\n",
    "    y_clean = y[mask]\n",
    "    \n",
    "    print(f\"Statistical analysis on {len(X_clean)} samples with {len(feature_names)} features\")\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_clean)\n",
    "    \n",
    "    # Store feature stats for reference\n",
    "    feature_stats = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Mean': scaler.mean_,\n",
    "        'Std': scaler.scale_\n",
    "    })\n",
    "    \n",
    "    # Fit logistic regression using statsmodels\n",
    "    X_with_intercept = sm.add_constant(X_scaled)\n",
    "    model = sm.Logit(y_clean, X_with_intercept)\n",
    "    results = model.fit(disp=0)\n",
    "    \n",
    "    # Extract results (excluding intercept)\n",
    "    coef = results.params[1:]\n",
    "    p_values = results.pvalues[1:]\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    stats_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient': coef,\n",
    "        'Std Error': results.bse[1:],\n",
    "        'z-value': results.tvalues[1:],\n",
    "        'P-value': p_values,\n",
    "        'Original Mean': scaler.mean_,\n",
    "        'Original Std': scaler.scale_\n",
    "    })\n",
    "    \n",
    "    # Sort by absolute coefficient value\n",
    "    stats_df = stats_df.assign(Abs_Coef=abs(stats_df['Coefficient'])).sort_values(\n",
    "        'Abs_Coef', ascending=False).drop('Abs_Coef', axis=1)\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "def get_feature_sets(variant):\n",
    "    \"\"\"variant should be 'vanilla' or 'robust'\"\"\"\n",
    "    return {\n",
    "        'C-Score': [f'cscore_{variant}'],\n",
    "        'Pred. Depth': [f'pred_depth_{variant}'],\n",
    "        'Adv. Robustness': [f'adv_eps_{variant}'],\n",
    "        'L$_{gt}$': [f'{variant}_gt_logit'],\n",
    "        'Combined w/o L$_{gt}$': [\n",
    "            f'cscore_{variant}',\n",
    "            f'pred_depth_{variant}',\n",
    "            f'adv_eps_{variant}'\n",
    "        ],\n",
    "        'All features': [\n",
    "            f'cscore_{variant}',\n",
    "            f'pred_depth_{variant}',\n",
    "            f'adv_eps_{variant}',\n",
    "            f'{variant}_gt_logit'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Perform analyses\n",
    "results = []\n",
    "statistical_results = {}\n",
    "\n",
    "for variant in ['vanilla', 'robust']:\n",
    "    print(variant.upper() + \" analysis\")\n",
    "    feature_sets = get_feature_sets(variant)\n",
    "    \n",
    "    for feature_name, feature_list in feature_sets.items():\n",
    "        # Get the data\n",
    "        X = df_combined_i16_nat_strict[feature_list].values\n",
    "        y = df_combined_i16_nat_strict['perf']\n",
    "        \n",
    "        print(f\"\\nAnalyzing {feature_name}\")\n",
    "        print(f\"Initial shape: {X.shape}\")\n",
    "        \n",
    "        mean_auc, ci_lower, ci_upper, n_samples = perform_cv_analysis(X, y, feature_name)\n",
    "        results.append((feature_name, mean_auc, ci_lower, ci_upper))\n",
    "        \n",
    "        # Perform statistical analysis for all feature combinations\n",
    "        if len(feature_list) > 1:  # Only perform statistical analysis for multiple features\n",
    "            try:\n",
    "                key = f\"{feature_name}\"  # Use the full feature name as the key\n",
    "                stat_results = perform_statistical_analysis(X, y, feature_list)\n",
    "                statistical_results[key] = stat_results\n",
    "            except Exception as e:\n",
    "                print(f\"Error in statistical analysis for {feature_name}:\")\n",
    "                print(str(e))\n",
    "\n",
    "# Create visualization\n",
    "plt.style.use('seaborn-v0_8-white')\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create hatching pattern for robust bars\n",
    "hatch_pattern = '//'\n",
    "\n",
    "# Separate vanilla and robust results\n",
    "vanilla_results = results[:6]\n",
    "robust_results = results[6:]\n",
    "\n",
    "# Calculate bar positions\n",
    "bar_width = 0.8\n",
    "n_bars = len(vanilla_results)\n",
    "vanilla_positions = np.arange(n_bars)\n",
    "robust_positions = np.arange(n_bars) + n_bars + 1\n",
    "\n",
    "# Plot vanilla results\n",
    "feature_names = [r[0].replace(' (vanilla)', '') for r in vanilla_results]\n",
    "mean_aucs = [r[1] for r in vanilla_results]\n",
    "errors = [[r[1] - r[2], r[3] - r[1]] for r in vanilla_results]\n",
    "\n",
    "bars1 = ax.bar(vanilla_positions, mean_aucs, bar_width, yerr=np.array(errors).T, capsize=5,\n",
    "               color='lightskyblue', edgecolor='black', linewidth=2,\n",
    "               error_kw=dict(lw=2, capthick=2), label='Vanilla')\n",
    "\n",
    "# Plot robust results\n",
    "mean_aucs = [r[1] for r in robust_results]\n",
    "errors = [[r[1] - r[2], r[3] - r[1]] for r in robust_results]\n",
    "\n",
    "bars2 = ax.bar(robust_positions, mean_aucs, bar_width, yerr=np.array(errors).T, capsize=5,\n",
    "               color='lightcoral', edgecolor='black', linewidth=2,\n",
    "               error_kw=dict(lw=2, capthick=2), label='Robust',\n",
    "               hatch=hatch_pattern)\n",
    "\n",
    "# Add vertical dotted line between groups\n",
    "midpoint = (vanilla_positions[-1] + robust_positions[0]) / 2\n",
    "ax.axvline(x=midpoint, color='black', linestyle=':', linewidth=2)\n",
    "\n",
    "# Customize plot\n",
    "ax.set_ylabel('AUC (predict human correct or not)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Feature Sets for Logistic Regression', fontsize=14, fontweight='bold')\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax.set_ylim(0.5, 0.75)\n",
    "\n",
    "# Set x-tick positions and labels\n",
    "all_positions = np.concatenate([vanilla_positions, robust_positions])\n",
    "offset = 0.2\n",
    "plt.xticks(all_positions + offset, feature_names * 2, rotation=45, ha='right')\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='upper left', frameon=True, framealpha=1.0, fontsize=12)\n",
    "\n",
    "# Remove top and right spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(2)\n",
    "ax.spines['bottom'].set_linewidth(2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"notebooks/fig_outputs/difficulty_prediction_systematic.pdf\", dpi=300, format=\"pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "# Print results\n",
    "print(\"\\nCross-validation Results:\")\n",
    "print(\"-\" * 50)\n",
    "for feature_name, mean_auc, ci_lower, ci_upper in results:\n",
    "    print(f\"\\n{feature_name}:\")\n",
    "    print(f\"  Mean AUC: {mean_auc:.3f}\")\n",
    "    print(f\"  95% CI: [{ci_lower:.3f}, {ci_upper:.3f}]\")\n",
    "\n",
    "print(\"\\nStatistical Analysis Results:\")\n",
    "print(\"-\" * 50)\n",
    "for feature_name, stat_results in statistical_results.items():\n",
    "    print(f\"\\n{feature_name} Statistical Analysis:\")\n",
    "    print(\"(Coefficients are standardized - features scaled to mean=0, std=1)\")\n",
    "    print(stat_results.to_string(float_format=lambda x: '{:.4f}'.format(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix Fig. S7 (predicting difficulty using ground truth logits from different models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "def analyze_model_logits(nat_df, models, output_path='notebooks/fig_outputs/logit_prediction_model_comparison.pdf'):\n",
    "    \"\"\"\n",
    "    Analyze and compare model logits using cross-validated logistic regression.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    nat_df : pandas.DataFrame\n",
    "        DataFrame containing human response data\n",
    "    models : list of dict\n",
    "        Each dict should contain:\n",
    "            - 'model_name': str, name of the model\n",
    "            - 'logit_df_path': str, path to CSV containing logit values\n",
    "    output_path : str\n",
    "        Path where to save the resulting plot\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (DataFrame with numerical results, matplotlib figure, matplotlib axis)\n",
    "    \"\"\"\n",
    "    \n",
    "    def bootstrap_ci(data, num_bootstrap_samples=10000, confidence_level=0.95):\n",
    "        bootstrap_means = np.array([np.mean(np.random.choice(data, size=len(data), replace=True)) \n",
    "                                  for _ in range(num_bootstrap_samples)])\n",
    "        return np.percentile(bootstrap_means, \n",
    "                           [(1 - confidence_level) / 2 * 100, \n",
    "                            (1 + confidence_level) / 2 * 100])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Analyze each model\n",
    "    for model_info in models:\n",
    "        model_name = model_info['model_name']\n",
    "        print(f\"\\nAnalyzing {model_name}...\")\n",
    "        \n",
    "        # Load logit dataframe\n",
    "        logit_df = pd.read_csv(model_info['logit_df_path'])\n",
    "        \n",
    "        # Create temporary dataframe for analysis\n",
    "        temp_df = nat_df.copy()\n",
    "        \n",
    "        # Map logit values\n",
    "        def find_logit_value_and_type_by_url(url, df):\n",
    "            filtered_df = df[df['url'] == url]\n",
    "            if len(filtered_df) > 1:\n",
    "                raise ValueError(f\"Multiple rows found for URL in {model_name}\")\n",
    "            elif len(filtered_df) == 0:\n",
    "                return None, None\n",
    "            else:\n",
    "                row = filtered_df.iloc[0]\n",
    "                # Check which logit type is available\n",
    "                if 'robust_gt_logit' in row and not pd.isna(row['robust_gt_logit']):\n",
    "                    return row['robust_gt_logit'], 'robust'\n",
    "                elif 'vanilla_gt_logit' in row and not pd.isna(row['vanilla_gt_logit']):\n",
    "                    return row['vanilla_gt_logit'], 'vanilla'\n",
    "                else:\n",
    "                    raise ValueError(f\"No valid logit found for {model_name}\")\n",
    "\n",
    "        # Add logit values to temporary dataframe\n",
    "        logit_values = []\n",
    "        logit_types = []\n",
    "        \n",
    "        for _, row in temp_df.iterrows():\n",
    "            value, logit_type = find_logit_value_and_type_by_url(\n",
    "                replace_bucket_name_in_url(row[\"stimulus_image_url\"].split(\"?\")[0], 'morgan-imagenet16'), \n",
    "                logit_df\n",
    "            )\n",
    "            logit_values.append(value)\n",
    "            logit_types.append(logit_type)\n",
    "        \n",
    "        temp_df['logit'] = logit_values\n",
    "        # Store the most common logit type used for this model\n",
    "        logit_type_used = max(set(filter(None, logit_types)), key=logit_types.count)\n",
    "        print(f\"{model_name}: Using {logit_type_used} logits\")\n",
    "        \n",
    "        # Remove rows with missing values\n",
    "        mask = ~np.isnan(temp_df['logit'])\n",
    "        X = temp_df[mask]['logit'].values.reshape(-1, 1)\n",
    "        y = temp_df[mask]['perf'].values\n",
    "        \n",
    "        # Print class distribution\n",
    "        class_counts = np.bincount(y.astype(int))\n",
    "        print(f\"Class distribution - {class_counts[0]} incorrect, {class_counts[1]} correct\")\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv = StratifiedKFold(n_splits=500, shuffle=True, random_state=42)\n",
    "        log_reg = LogisticRegression()\n",
    "        auc_scores = cross_val_score(log_reg, X_scaled, y, cv=cv, scoring='roc_auc')\n",
    "        \n",
    "        # Calculate statistics\n",
    "        mean_auc = np.mean(auc_scores)\n",
    "        ci_lower, ci_upper = bootstrap_ci(auc_scores)\n",
    "        \n",
    "        results.append({\n",
    "            'model': model_name,\n",
    "            'mean_auc': mean_auc,\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper,\n",
    "            'n_samples': len(X),\n",
    "            'logit_type': logit_type_used\n",
    "        })\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.style.use('seaborn-v0_8-white')\n",
    "    plt.rcParams['ytick.left'] = True\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Calculate bar positions and width\n",
    "    x = np.arange(len(results))\n",
    "    width = 0.8\n",
    "    \n",
    "    # Create bars with different styles based on logit type\n",
    "    vanilla_bar = None\n",
    "    robust_bar = None\n",
    "    \n",
    "    for i, row in results_df.iterrows():\n",
    "        color = 'lightcoral' if row['logit_type'] == 'robust' else 'lightskyblue'\n",
    "        hatch = '//' if row['logit_type'] == 'robust' else ''\n",
    "        \n",
    "        bar = ax.bar(i, row['mean_auc'], width,\n",
    "                    yerr=[[row['mean_auc'] - row['ci_lower']], [row['ci_upper'] - row['mean_auc']]],\n",
    "                    capsize=5,\n",
    "                    color=color,\n",
    "                    edgecolor='black',\n",
    "                    linewidth=2,\n",
    "                    error_kw=dict(lw=2, capthick=2),\n",
    "                    hatch=hatch)\n",
    "        \n",
    "        # Store reference to bars for legend\n",
    "        if row['logit_type'] == 'vanilla' and vanilla_bar is None:\n",
    "            vanilla_bar = bar\n",
    "        elif row['logit_type'] == 'robust' and robust_bar is None:\n",
    "            robust_bar = bar\n",
    "    \n",
    "    # Create legend handles\n",
    "    legend_elements = []\n",
    "    if vanilla_bar is not None:\n",
    "        legend_elements.append(vanilla_bar)\n",
    "    if robust_bar is not None:\n",
    "        legend_elements.append(robust_bar)\n",
    "    \n",
    "    # Add legend if we have both types\n",
    "    if len(legend_elements) > 1:\n",
    "        ax.legend(legend_elements, ['Non-adversarially trained models', 'Adversarially trained models'],\n",
    "                 loc='upper left', frameon=False, framealpha=1.0, fontsize=14)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_ylabel('AUC (predict human correct or not)', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Models', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Set tick parameters\n",
    "    ax.tick_params(axis='both',which='major', labelsize=14)\n",
    "    ax.tick_params(axis='y', which='major', length=6, width=2)\n",
    "    \n",
    "    ax.set_ylim([0.5, 0.75])\n",
    "    \n",
    "    # Set x-tick positions and labels\n",
    "    plt.xticks(x, results_df['model'], rotation=45, ha='right')\n",
    "    \n",
    "    # Remove top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_linewidth(2)\n",
    "    ax.spines['bottom'].set_linewidth(2)\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, format=\"pdf\", bbox_inches=\"tight\")\n",
    "    \n",
    "    # Print numerical results\n",
    "    print(\"\\nNumerical Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    for _, row in results_df.iterrows():\n",
    "        print(f\"\\n{row['model']} ({row['logit_type']} logits):\")\n",
    "        print(f\"  Mean AUC: {row['mean_auc']:.3f}\")\n",
    "        print(f\"  95% CI: [{row['ci_lower']:.3f}, {row['ci_upper']:.3f}]\")\n",
    "        print(f\"  N samples: {row['n_samples']}\")\n",
    "    \n",
    "    return results_df, fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "  {\"model_name\": \"Vanilla RN50\", \"logit_df_path\": \"psych_data/dataset_dirmaps/imagenet_animals_different_models/dataset_dirmap_logits_resnet50.csv\"},\n",
    "  {\"model_name\": \"CutMix RN50\", \"logit_df_path\": \"psych_data/dataset_dirmaps/imagenet_animals_different_models/dataset_dirmap_logits_resnet50_cutmix.csv\"},\n",
    "  {\"model_name\": \"ϵ = 1 RN50\", \"logit_df_path\": \"psych_data/dataset_dirmaps/imagenet_animals_different_models/dataset_dirmap_logits_resnet50_eps1.csv\"},\n",
    "  {\"model_name\": \"ϵ = 3 RN50\", \"logit_df_path\": \"psych_data/dataset_dirmaps/imagenet_animals_different_models/dataset_dirmap_logits_resnet50_eps3.csv\"},\n",
    "  {\"model_name\": \"ϵ = 10 RN50\", \"logit_df_path\": \"psych_data/dataset_dirmaps/imagenet_animals_different_models/dataset_dirmap_logits_resnet50_eps10.csv\"},\n",
    "  {\"model_name\": \"ϵ = 4 XCiT\", \"logit_df_path\": \"psych_data/dataset_dirmaps/imagenet_animals_different_models/dataset_dirmap_logits_xcit_large.csv\"},\n",
    "]\n",
    "\n",
    "analyze_model_logits(df_combined_i16_nat_strict, models, output_path='notebooks/fig_outputs/logit_prediction_model_comparison.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix Fig. S8: analysis of image difficulty predictions from epoch-wise robust model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_epoch_logits(nat_df, dirmap_path, accuracy_csv_path, output_path='notebooks/fig_outputs/epoch_logit_prediction.pdf'):\n",
    "    \"\"\"\n",
    "    Analyze how model logits from different training epochs predict human performance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    nat_df : pandas.DataFrame\n",
    "        DataFrame containing human response data\n",
    "    dirmap_path : str\n",
    "        Path to CSV containing epoch logit values (epoch0_gt_logit, epoch1_gt_logit, etc.)\n",
    "    accuracy_csv_path : str\n",
    "        Path to CSV containing model training and validation accuracy per epoch\n",
    "    output_path : str\n",
    "        Path where to save the resulting plot\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (DataFrame with numerical results, matplotlib figure)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    # Bootstrap CI calculation function (same as in original)\n",
    "    def bootstrap_ci(data, num_bootstrap_samples=10000, confidence_level=0.95):\n",
    "        bootstrap_means = np.array([np.mean(np.random.choice(data, size=len(data), replace=True)) \n",
    "                                  for _ in range(num_bootstrap_samples)])\n",
    "        return np.percentile(bootstrap_means, \n",
    "                           [(1 - confidence_level) / 2 * 100, \n",
    "                            (1 + confidence_level) / 2 * 100])\n",
    "    \n",
    "    # Load dataframes\n",
    "    logit_df = pd.read_csv(dirmap_path)\n",
    "    accuracy_df = pd.read_csv(accuracy_csv_path)\n",
    "    \n",
    "    # Ensure accuracy dataframe is sorted by epoch\n",
    "    accuracy_df = accuracy_df.sort_values('epoch').reset_index(drop=True)\n",
    "    \n",
    "    # Identify epoch columns in the dirmap file\n",
    "    epoch_columns = [col for col in logit_df.columns if col.startswith('epoch') and col.endswith('_gt_logit')]\n",
    "    \n",
    "    # Check if we found any epoch columns\n",
    "    if not epoch_columns:\n",
    "        raise ValueError(\"No epoch_*_gt_logit columns found in the dirmap CSV file.\")\n",
    "    \n",
    "    # Check if accuracy_df has the required columns\n",
    "    required_columns = ['epoch', 'train_acc', 'val_acc']\n",
    "    missing_columns = [col for col in required_columns if col not in accuracy_df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Accuracy CSV missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # Sort the epoch columns by epoch number\n",
    "    epoch_columns.sort(key=lambda x: int(x.replace('epoch', '').replace('_gt_logit', '')))\n",
    "    \n",
    "    # Store results for each epoch\n",
    "    results = []\n",
    "    \n",
    "    # Process each epoch's logits\n",
    "    for epoch_col in epoch_columns:\n",
    "        epoch_num = int(epoch_col.replace('epoch', '').replace('_gt_logit', ''))\n",
    "        print(f\"\\nAnalyzing epoch {epoch_num}...\")\n",
    "        \n",
    "        # Create temporary dataframe for analysis\n",
    "        temp_df = nat_df.copy()\n",
    "        \n",
    "        # Map logit values for this epoch\n",
    "        logit_values = []\n",
    "        \n",
    "        for _, row in temp_df.iterrows():\n",
    "            # Find matching URL in logit_df\n",
    "            url = replace_bucket_name_in_url(row[\"stimulus_image_url\"].split(\"?\")[0], 'morgan-imagenet16')\n",
    "            matching_row = logit_df[logit_df['url'] == url]\n",
    "            \n",
    "            if len(matching_row) > 0:\n",
    "                logit_value = matching_row[epoch_col].values[0]\n",
    "            else:\n",
    "                logit_value = np.nan\n",
    "                \n",
    "            logit_values.append(logit_value)\n",
    "        \n",
    "        temp_df['logit'] = logit_values\n",
    "        \n",
    "        # Remove rows with missing values\n",
    "        mask = ~np.isnan(temp_df['logit'])\n",
    "        X = temp_df[mask]['logit'].values.reshape(-1, 1)\n",
    "        y = temp_df[mask]['perf'].values\n",
    "        \n",
    "        # Print class distribution\n",
    "        class_counts = np.bincount(y.astype(int))\n",
    "        print(f\"Class distribution - {class_counts[0]} incorrect, {class_counts[1]} correct\")\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv = StratifiedKFold(n_splits=500, shuffle=True, random_state=42)\n",
    "        log_reg = LogisticRegression(max_iter=1000)\n",
    "        auc_scores = cross_val_score(log_reg, X_scaled, y, cv=cv, scoring='roc_auc')\n",
    "        \n",
    "        # Calculate statistics\n",
    "        mean_auc = np.mean(auc_scores)\n",
    "        ci_lower, ci_upper = bootstrap_ci(auc_scores)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'epoch': epoch_num,\n",
    "            'mean_auc': mean_auc,\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper,\n",
    "            'n_samples': len(X)\n",
    "        })\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Create visualization with two y-axes\n",
    "    plt.style.use('seaborn-v0_8-white')\n",
    "    \n",
    "    # Enable tick marks on all axes\n",
    "    plt.rcParams['ytick.left'] = True\n",
    "    plt.rcParams['ytick.right'] = True\n",
    "    plt.rcParams['xtick.bottom'] = True\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Primary y-axis: AUC for human performance prediction\n",
    "    ax1.set_xlabel('Training epoch', fontsize=18, fontweight='bold')\n",
    "    ax1.set_ylabel('AUC (predict human error rate)', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # Plot AUC values with confidence intervals\n",
    "    line1, = ax1.plot(results_df['epoch'], results_df['mean_auc'], 'o-', color='navy', \n",
    "             linewidth=2, markersize=5, label='Human error rate prediction (AUC)')\n",
    "    ax1.fill_between(results_df['epoch'], \n",
    "                     results_df['ci_lower'], \n",
    "                     results_df['ci_upper'], \n",
    "                     alpha=0.2, color='navy')\n",
    "    \n",
    "    # Set y-axis range for AUC\n",
    "    ax1.set_ylim([0.5, 0.75])\n",
    "    \n",
    "    # Secondary y-axis: Model training/validation accuracy\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('ImageNet classification accuracy', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # Plot training and validation accuracy with updated labels\n",
    "    line2, = ax2.plot(accuracy_df['epoch'], accuracy_df['train_acc'], 's-', color='forestgreen', \n",
    "             linewidth=2, markersize=5, label='ImageNet training accuracy')\n",
    "    line3, = ax2.plot(accuracy_df['epoch'], accuracy_df['val_acc'], '^-', color='crimson', \n",
    "             linewidth=2, markersize=6, label='ImageNet validation accuracy')\n",
    "    \n",
    "    # Set y-axis range for model accuracy\n",
    "    ax2.set_ylim([0, 1.0])\n",
    "    \n",
    "    # Combine legends from both axes\n",
    "    ax1.legend([line1, line2, line3], \n",
    "           ['Human error rate prediction (AUC)', \n",
    "            'ImageNet training accuracy', \n",
    "            'ImageNet validation accuracy'], \n",
    "           loc='lower right', frameon=True, fontsize=16)  # Larger font and bottom right position\n",
    "    \n",
    "    # Increase tick label font sizes\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax1.tick_params(axis='both', which='both', length=6, width=1.5)\n",
    "    ax2.tick_params(axis='both', which='both', length=6, width=1.5)\n",
    "    \n",
    "    # Style customization\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(True)\n",
    "    ax1.spines['left'].set_linewidth(2)\n",
    "    ax1.spines['bottom'].set_linewidth(2)\n",
    "    ax2.spines['right'].set_linewidth(2)\n",
    "    \n",
    "    # Add grid for easier reading\n",
    "    ax1.grid(True, axis='y', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Adjust x-axis to show integer ticks at appropriate intervals\n",
    "    max_epoch = max(results_df['epoch'])\n",
    "    if max_epoch <= 20:\n",
    "        tick_spacing = 1\n",
    "    elif max_epoch <= 50:\n",
    "        tick_spacing = 5\n",
    "    else:\n",
    "        tick_spacing = 10\n",
    "        \n",
    "    ax1.set_xticks(range(0, max_epoch + 1, tick_spacing))\n",
    "\n",
    "    plt.rcParams['xtick.direction'] = 'out'\n",
    "    plt.rcParams['ytick.direction'] = 'out'\n",
    "    plt.rcParams['xtick.major.size'] = 6\n",
    "    plt.rcParams['ytick.major.size'] = 6\n",
    "    \n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "    \n",
    "    # Print numerical results\n",
    "    print(\"\\nNumerical Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    for _, row in results_df.iterrows():\n",
    "        print(f\"\\nEpoch {row['epoch']}:\")\n",
    "        print(f\"  Mean AUC: {row['mean_auc']:.3f}\")\n",
    "        print(f\"  95% CI: [{row['ci_lower']:.3f}, {row['ci_upper']:.3f}]\")\n",
    "        print(f\"  N samples: {row['n_samples']}\")\n",
    "    \n",
    "    return results_df, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_epoch_logits(\n",
    "    nat_df=df_combined_i16_nat_strict, \n",
    "    dirmap_path=\"psych_data/imagenet_animals_model_epochs_analysis/imagenet_animals_90epochs_logits_dirmap.csv\", \n",
    "    accuracy_csv_path=\"psych_data/imagenet_animals_model_epochs_analysis/epoch_wise_acc.csv\", \n",
    "    output_path='notebooks/fig_outputs/epoch_logit_prediction.pdf'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix Fig. S9 (comparing image enhancement with different guide models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import AutoMinorLocator\n",
    "\n",
    "def create_model_comparison_plot(df, normalize=False, output_path=None):\n",
    "    \"\"\"\n",
    "    Create a bar plot comparing model performances with confidence intervals.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing columns: 'participant', 'trial_type', 'perf'\n",
    "    normalize : bool, default=False\n",
    "        Whether to normalize performance against natural trial performance\n",
    "    output_path : str, optional\n",
    "        Path to save the figure. If None, figure is only displayed\n",
    "    \"\"\"\n",
    "    # Calculate normalized performance if requested\n",
    "    if normalize:\n",
    "        natural_means = df[df['trial_type'] == 'natural'].groupby('participant')['perf'].mean()\n",
    "        df = df.copy()  # Create copy to avoid modifying original\n",
    "        df['normalized_perf'] = df.apply(\n",
    "            lambda row: row['perf'] / natural_means[row['participant']], \n",
    "            axis=1\n",
    "        )\n",
    "        perf_metric = 'normalized_perf'\n",
    "    else:\n",
    "        perf_metric = 'perf'\n",
    "\n",
    "    # Define trial types to include\n",
    "    trial_types = [\n",
    "        'natural',\n",
    "        'enhanced_vanilla_resnet50',\n",
    "        'enhanced_cutmix_resnet50',\n",
    "        'enhanced_eps1_resnet50',\n",
    "        'enhanced_eps3_resnet50',\n",
    "        'enhanced_eps10_resnet50',\n",
    "        'enhanced_xcit_augmented',\n",
    "    ]\n",
    "\n",
    "    # Filter and calculate group statistics\n",
    "    mask = df['trial_type'].isin(trial_types)\n",
    "    accuracy_by_group = df[mask].groupby(['participant', 'trial_type'])[perf_metric].mean()\n",
    "\n",
    "    # Calculate means and confidence intervals\n",
    "    def bootstrap_ci(data, num_samples=10000, ci=0.95):\n",
    "        if len(data) < 2:  # Check for insufficient data\n",
    "            return np.nan, np.nan\n",
    "        bootstrap_means = np.random.choice(data, (num_samples, len(data)), replace=True).mean(axis=1)\n",
    "        return np.percentile(bootstrap_means, [(1-ci)/2*100, (1+ci)/2*100])\n",
    "\n",
    "    results = []\n",
    "    for trial_type in trial_types:\n",
    "        data = accuracy_by_group.xs(trial_type, level='trial_type')\n",
    "        mean_val = data.mean()\n",
    "        ci_lower, ci_upper = bootstrap_ci(data)\n",
    "        results.append({\n",
    "            'trial_type': trial_type,\n",
    "            'mean': mean_val,\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper\n",
    "        })\n",
    "    \n",
    "    result_df = pd.DataFrame(results)\n",
    "\n",
    "    # Plotting\n",
    "    plt.style.use('seaborn-v0_8-white')\n",
    "    sns.set_palette(\"deep\")\n",
    "    plt.rcParams['ytick.left'] = True\n",
    "    \n",
    "    # Create figure with higher DPI\n",
    "    fig, ax = plt.subplots(figsize=(12, 6), dpi=300)\n",
    "\n",
    "    # Define x positions and width\n",
    "    x = np.arange(len(trial_types))\n",
    "    width = 0.6\n",
    "\n",
    "    # Calculate error bar heights\n",
    "    means = result_df['mean']\n",
    "    yerr = np.array([\n",
    "        means - result_df['ci_lower'],\n",
    "        result_df['ci_upper'] - means\n",
    "    ])\n",
    "\n",
    "    # Create bars\n",
    "    bars = ax.bar(x, means, width, \n",
    "                 yerr=yerr, \n",
    "                 capsize=10,\n",
    "                 alpha=0.8, \n",
    "                 edgecolor='black', \n",
    "                 linewidth=2,\n",
    "                 error_kw={'elinewidth': 2, 'capthick': 2})\n",
    "\n",
    "    # Customize labels and formatting\n",
    "    ylabel = 'Normalized rate humans choose\\nground truth [%]' if normalize else 'Rate humans choose\\nground truth [%]'\n",
    "    ax.set_ylabel(ylabel, fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel(\"Guide Models\", fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Define and set x-tick labels\n",
    "    xtick_labels = {\n",
    "        'natural': 'Original',\n",
    "        'enhanced_vanilla_resnet50': 'Vanilla RN50',\n",
    "        'enhanced_cutmix_resnet50': 'CutMix RN50',\n",
    "        'enhanced_eps1_resnet50': 'ϵ = 1 RN50',\n",
    "        'enhanced_eps3_resnet50': 'ϵ = 3 RN50',\n",
    "        'enhanced_eps10_resnet50': 'ϵ = 10 RN50',\n",
    "        'enhanced_xcit_augmented': 'ϵ = 4 XCiT'\n",
    "    }\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(\n",
    "        [xtick_labels[t] for t in trial_types],\n",
    "        fontsize=14,\n",
    "        rotation=45,\n",
    "        ha='right'\n",
    "    )\n",
    "\n",
    "    # Set axis limits and format\n",
    "    if normalize:\n",
    "        ax.set_ylim(0.8, 1.2)\n",
    "    else:\n",
    "        ax.set_ylim(0.7, 0.95)\n",
    "\n",
    "    # Add minor ticks\n",
    "    ax.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "    # Add reference line for natural condition\n",
    "    natural_mean = means[result_df['trial_type'] == 'natural'].iloc[0]\n",
    "    ax.axhline(\n",
    "        y=natural_mean,\n",
    "        color='red',\n",
    "        linestyle=':',\n",
    "        linewidth=2,\n",
    "        label='Mean acc. on unmodified images'\n",
    "    )\n",
    "\n",
    "    # Customize spines and ticks\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_linewidth(1.5)\n",
    "    ax.spines['bottom'].set_linewidth(1.5)\n",
    "    ax.tick_params(axis='y', which='major', length=6, width=2, labelsize=14)\n",
    "    ax.tick_params(axis='y', which='both', right=False)\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend(fontsize=14, frameon=False, edgecolor='black', loc='upper left')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save if path provided\n",
    "    if output_path:\n",
    "        # Ensure the figure is rendered before saving\n",
    "        fig.canvas.draw()\n",
    "        plt.savefig(output_path, dpi=300, format='pdf', bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "create_model_comparison_plot(df_backbone_compare_i16, normalize=False, output_path='notebooks/fig_outputs/guide_model_enhancement_comparison.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplementary Fig. S11 (ablation study on image enhancement with ImageNet animal images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-white')\n",
    "sns.set_palette(\"deep\")\n",
    "\n",
    "NORMALIZE = False\n",
    "df_s8 = df_loss_ablation_i16.copy()\n",
    "\n",
    "# First, calculate the mean 'perf' for each participant on 'natural' trials\n",
    "natural_means = df_s8[df_s8['trial_type'] == 'natural'].groupby('participant')['perf'].mean()\n",
    "\n",
    "# Create a function to normalize performance\n",
    "def normalize_performance(row):\n",
    "    return row['perf'] / natural_means[row['participant']]\n",
    "\n",
    "# Apply the normalization to all rows\n",
    "df_s8['normalized_perf'] = df_s8.apply(normalize_performance, axis=1)\n",
    "perf_metric = 'normalized_perf' if NORMALIZE else 'perf'\n",
    "\n",
    "# Step 1: Filter out 'calibration' and 'new_stimulus' trial types\n",
    "filtered_df_s8 = df_s8[~df_s8['trial_type'].isin(['calibration', 'new_stimulus'])]\n",
    "\n",
    "trial_types_s8 = np.flip(filtered_df_s8['trial_type'].unique())\n",
    "\n",
    "# Step 2: Group by trial_type and split, and calculate mean accuracy and bootstrap CI for each group\n",
    "def calculate_bootstrap_for_group(group):\n",
    "    mean_val = group[perf_metric].mean()\n",
    "    ci_lower, ci_upper = bootstrap_ci(group[perf_metric])\n",
    "    return pd.Series({'mean': mean_val, 'ci_lower': ci_lower, 'ci_upper': ci_upper})\n",
    "\n",
    "# Apply the function to each group\n",
    "grouped_results = filtered_df_s8.groupby(['trial_type', 'split']).apply(calculate_bootstrap_for_group).reset_index()\n",
    "\n",
    "pivot_result = grouped_results.pivot(index='split', columns='trial_type', values=['mean', 'ci_lower', 'ci_upper'])\n",
    "\n",
    "# Increase the figure size and DPI for better quality\n",
    "fig, ax = plt.subplots(figsize=(12, 5), dpi=300)\n",
    "\n",
    "trial_type_mapping = {\n",
    "    'natural': 'Natural',\n",
    "    'enhanced_logit': 'Enhanced (Logit)',\n",
    "    'enhanced_cross_entropy': 'Enhanced (Cross-entropy)',\n",
    "}\n",
    "trial_types_disp = [trial_type_mapping.get(t, t) for t in trial_types_s8]\n",
    "\n",
    "splits = [\"val\", \"train\"]\n",
    "\n",
    "split_label_mapping = {\n",
    "    'val': 'ImageNet Validation Set',\n",
    "    'train': 'ImageNet Training Set',\n",
    "}\n",
    "\n",
    "# Define hatch patterns for texture coding\n",
    "hatch_patterns = ['', '///', '...']\n",
    "\n",
    "# Plot bars for each trial type, grouped by split\n",
    "x = np.arange(len(splits))\n",
    "width = 0.25  # Adjust bar width\n",
    "for i, trial_type in enumerate(trial_types_s8):\n",
    "    means = pivot_result['mean'][trial_type].values\n",
    "    ci_lower = pivot_result['ci_lower'][trial_type].values\n",
    "    ci_upper = pivot_result['ci_upper'][trial_type].values\n",
    "    yerr = np.array([means - ci_lower, ci_upper - means])\n",
    "    ax.bar(x + (i - 1)*width, means, width, yerr=yerr, capsize=5, \n",
    "           label=trial_types_disp[i], alpha=0.8, edgecolor='black', linewidth=2, \n",
    "           error_kw={'elinewidth': 2, 'capthick': 2}, hatch=hatch_patterns[i])\n",
    "\n",
    "# Customize the plot\n",
    "if NORMALIZE:\n",
    "    ylabel = 'Normalized Mean Accuracy'\n",
    "else:\n",
    "    ylabel = 'Mean Accuracy'\n",
    "ax.set_ylabel(ylabel, fontsize=20, fontweight='bold')\n",
    "ax.legend(fontsize=16, frameon=True, edgecolor='black', loc='upper left')\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7, color='gray')\n",
    "\n",
    "# Set x-ticks and labels\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([split_label_mapping[split] for split in splits], fontsize=20, fontweight='bold')\n",
    "\n",
    "# Increase tick label size\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "# Add a horizontal line at y=1 to represent the baseline (natural trial type)\n",
    "if NORMALIZE:\n",
    "    ax.axhline(y=1, color='red', linestyle='--', linewidth=2, label='Natural Trial Baseline')\n",
    "\n",
    "# Set y-axis limits\n",
    "if NORMALIZE:\n",
    "    ax.set_ylim(0.8, 1.2)\n",
    "else:\n",
    "    ax.set_ylim(0.675, 0.875)\n",
    "\n",
    "# Add subtle spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(1.5)\n",
    "ax.spines['bottom'].set_linewidth(1.5)\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "# Optionally, save the figure as a high-resolution image\n",
    "plt.savefig('notebooks/fig_outputs/accuracy_by_split_plot.pdf', dpi=300, format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhance_type_for_interaction_plot = 'enhanced_logit_20'\n",
    "\n",
    "# Select only 'natural' and a enhanced trial types with specific enhancement level\n",
    "df_selected_s8b = df_main_i16[df_main_i16['trial_type'].isin(['natural', enhance_type_for_interaction_plot])]\n",
    "\n",
    "# Replace with the robust_gt_logit for the ORIGINAL image for this analysis (not for the enhanced image)\n",
    "df_selected_s8b[\"robust_gt_logit\"] = df_selected_s8b.apply(lambda row: find_value_by_url(replace_bucket_name_in_url(row[\"stimulus_image_url\"].split(\"?\")[0], \"morgan-imagenet16\"), logit_df, \"robust_gt_logit\"), axis=1)\n",
    "\n",
    "# Function to calculate difference in performance and its CI\n",
    "def calc_perf_diff_with_ci(natural_data, enhanced_data):\n",
    "    natural_mean = np.mean(natural_data['perf'])\n",
    "    enhanced_mean = np.mean(enhanced_data['perf'])\n",
    "    diff = enhanced_mean - natural_mean\n",
    "    \n",
    "    # Bootstrap for confidence interval\n",
    "    diff_samples = []\n",
    "    for _ in range(10000):\n",
    "        natural_sample = natural_data['perf'].sample(n=len(natural_data), replace=True)\n",
    "        enhanced_sample = enhanced_data['perf'].sample(n=len(enhanced_data), replace=True)\n",
    "        diff_samples.append(np.mean(enhanced_sample) - np.mean(natural_sample))\n",
    "    \n",
    "    ci_lower, ci_upper = np.percentile(diff_samples, [2.5, 97.5])\n",
    "    return diff, ci_lower, ci_upper\n",
    "\n",
    "# Calculate quartiles for robust_gt_logit\n",
    "quartiles = df_selected_s8b['robust_gt_logit'].quantile([0.25, 0.5, 0.75])\n",
    "\n",
    "print(\"quartiles:\")\n",
    "print(quartiles)\n",
    "\n",
    "# Initialize lists to store results\n",
    "quartile_labels = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "diff_means = []\n",
    "diff_ci_lower = []\n",
    "diff_ci_upper = []\n",
    "\n",
    "# Calculate differences for each quartile\n",
    "for i in range(4):\n",
    "    if i == 0:\n",
    "        mask = df_selected_s8b['robust_gt_logit'] <= quartiles.iloc[0]\n",
    "    elif i == 3:\n",
    "        mask = df_selected_s8b['robust_gt_logit'] > quartiles.iloc[2]\n",
    "    else:\n",
    "        mask = (df_selected_s8b['robust_gt_logit'] > quartiles.iloc[i-1]) & (df_selected_s8b['robust_gt_logit'] <= quartiles.iloc[i])\n",
    "    \n",
    "    natural_data = df_selected_s8b[(df_selected_s8b['trial_type'] == 'natural') & mask]\n",
    "    enhanced_data = df_selected_s8b[(df_selected_s8b['trial_type'] == enhance_type_for_interaction_plot) & mask]\n",
    "    \n",
    "    diff, ci_lower, ci_upper = calc_perf_diff_with_ci(natural_data, enhanced_data)\n",
    "    diff_means.append(diff)\n",
    "    diff_ci_lower.append(ci_lower)\n",
    "    diff_ci_upper.append(ci_upper)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(8, 5), dpi=300)\n",
    "orange_color = sns.color_palette('deep')[1]  # Get the orange color\n",
    "\n",
    "# Create the bar plot with edge color\n",
    "bars = ax.bar(quartile_labels, diff_means, color=orange_color, width=0.9, \n",
    "               edgecolor='black', linewidth=2, hatch='///')\n",
    "\n",
    "# Add error bars with increased linewidth\n",
    "ax.errorbar(quartile_labels, diff_means, \n",
    "             yerr=[np.array(diff_means) - np.array(diff_ci_lower), \n",
    "                   np.array(diff_ci_upper) - np.array(diff_means)],\n",
    "             fmt='none', color='black', capsize=5, capthick=2, elinewidth=2)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Quartiles of Groundtruth Logit', fontsize=20, fontweight='bold')\n",
    "plt.ylabel('Δ Accuracy ϵ = 0 → ϵ = 20', fontsize=20, fontweight='bold')\n",
    "plt.ylim([0, 0.30])\n",
    "plt.tick_params(axis='both', which='major', labelsize=16)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7, color='gray')\n",
    "\n",
    "# Add subtle spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(1.5)\n",
    "ax.spines['bottom'].set_linewidth(1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"notebooks/fig_outputs/quartile_difference_plot.pdf\", format=\"pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Print out the numerical results\n",
    "for i, label in enumerate(quartile_labels):\n",
    "    print(f\"{label}: Difference = {diff_means[i]:.3f}, 95% CI: [{diff_ci_lower[i]:.3f}, {diff_ci_upper[i]:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout probability analysis (Appendix Section S9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout analysis for idaea4 experiment\n",
    "\n",
    "Total dropouts: 9\n",
    "\n",
    "Dropouts in control condition: 6\n",
    "\n",
    "Probability of being assigned to control: 2/7 ~= 0.2857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/dropout_statistics.py --total 9 --condition 6 --n_conditions 6 --p_condition 0.2857 -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout analysis for ham4 experiment\n",
    "\n",
    "Total dropouts: 13\n",
    "\n",
    "Dropouts in control condition: 6\n",
    "\n",
    "(Note: 4 additional dropouts had been assigned to the shuffled enhancement taper condition)\n",
    "\n",
    "Probability of being assigned to control: (227/317)(1/6)  +  (90/317)(1/3) = 0.214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/dropout_statistics.py --total 13 --condition 6 --n_conditions 6 --p_condition 0.214 -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographics table generation (Appendix Section S11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for demographics calculations\n",
    "\n",
    "def clean_age(age, participant_id=None):\n",
    "    \"\"\"\n",
    "    Clean age values, handling invalid entries.\n",
    "    \n",
    "    Parameters:\n",
    "    age: Age value that could be string or numeric\n",
    "    participant_id: Identifier for the participant (for warning messages)\n",
    "    \n",
    "    Returns:\n",
    "    float or None: Cleaned age value or None if invalid\n",
    "    \"\"\"\n",
    "    if pd.isna(age):\n",
    "        if participant_id is not None:\n",
    "            warnings.warn(f\"Missing age value for participant {participant_id}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Convert to string first to handle any numeric types\n",
    "        age_str = str(age).strip()\n",
    "        # Convert to float and check if it's a reasonable age\n",
    "        age_float = float(age_str)\n",
    "        # Basic validation: age should be between 0 and 120\n",
    "        if 0 <= age_float <= 120:\n",
    "            return age_float\n",
    "        else:\n",
    "            if participant_id is not None:\n",
    "                warnings.warn(f\"Age value {age_float} outside reasonable range (0-120) for participant {participant_id}\")\n",
    "            return None\n",
    "    except (ValueError, TypeError):\n",
    "        if participant_id is not None:\n",
    "            warnings.warn(f\"Invalid age value '{age}' for participant {participant_id}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def demographic_analysis(trial_data, demog_data, trial_key='worker_id', demog_key='Participant id'):\n",
    "    \"\"\"\n",
    "    Analyze demographic characteristics of study participants with additional diagnostics for missing data.\n",
    "    \"\"\"\n",
    "    # Get unique worker IDs from trial data\n",
    "    unique_workers = trial_data[trial_key].unique()\n",
    "    \n",
    "    # Diagnostic information\n",
    "    print(f\"\\nDiagnostic Information:\")\n",
    "    print(f\"Total unique worker_ids in trial_data: {len(unique_workers)}\")\n",
    "    print(f\"Total rows in demographic data: {len(demog_data)}\")\n",
    "    \n",
    "    # Check which worker_ids don't have matching demographic data\n",
    "    missing_demographics = set(unique_workers) - set(demog_data[demog_key])\n",
    "    print(f\"Number of workers missing from demographic data: {len(missing_demographics)}\")\n",
    "    \n",
    "    if len(missing_demographics) > 0:\n",
    "        print(\"\\nSample of missing worker_ids (up to 5):\")\n",
    "        for worker_id in list(missing_demographics)[:5]:\n",
    "            print(f\"  {worker_id}\")\n",
    "            \n",
    "    # Check for any demographic data that doesn't match trial participants\n",
    "    extra_demographics = set(demog_data['Participant id']) - set(unique_workers)\n",
    "    if len(extra_demographics) > 0:\n",
    "        print(f\"\\nNumber of demographic entries not in trial_data: {len(extra_demographics)}\")\n",
    "    \n",
    "    # Original analysis continues...\n",
    "    study_demog = demog_data[demog_data[demog_key].isin(unique_workers)].copy()\n",
    "    \n",
    "    # Clean age data\n",
    "    study_demog['Age_clean'] = study_demog.apply(\n",
    "        lambda row: clean_age(row['Age'], row[demog_key]), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Clean Sex and Ethnicity data\n",
    "    study_demog['Sex'] = study_demog['Sex'].replace({\n",
    "        'Prefer not to say': 'Not specified',\n",
    "        'DATA_EXPIRED': 'Not specified'\n",
    "    })\n",
    "    \n",
    "    study_demog['Ethnicity simplified'] = study_demog['Ethnicity simplified'].replace({\n",
    "        'Prefer not to say': 'Not specified',\n",
    "        'DATA_EXPIRED': 'Not specified'\n",
    "    })\n",
    "    \n",
    "    results = {\n",
    "        'n_participants': len(unique_workers),\n",
    "        'n_with_demographics': len(study_demog),\n",
    "        'age': {},\n",
    "        'sex': {},\n",
    "        'ethnicity': {}\n",
    "    }\n",
    "    \n",
    "    # Age analysis (using cleaned age data)\n",
    "    valid_ages = study_demog['Age_clean'].dropna()\n",
    "    if len(valid_ages) > 0:\n",
    "        results['age'] = {\n",
    "            'n': len(valid_ages),\n",
    "            'mean': np.round(valid_ages.mean(), 1),\n",
    "            'std': np.round(valid_ages.std(), 1),\n",
    "            'range': f\"{int(valid_ages.min())}-{int(valid_ages.max())}\",\n",
    "            'missing': len(study_demog) - len(valid_ages)\n",
    "        }\n",
    "    else:\n",
    "        results['age'] = {\n",
    "            'n': 0,\n",
    "            'mean': None,\n",
    "            'std': None,\n",
    "            'range': None,\n",
    "            'missing': len(study_demog)\n",
    "        }\n",
    "    \n",
    "    # Sex distribution\n",
    "    sex_counts = study_demog['Sex'].value_counts()\n",
    "    sex_percentages = (sex_counts / len(study_demog) * 100).round(1)\n",
    "    results['sex'] = {\n",
    "        category: f\"{count} ({percentage}%)\"\n",
    "        for category, count, percentage in zip(\n",
    "            sex_counts.index,\n",
    "            sex_counts.values,\n",
    "            sex_percentages.values\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Ethnicity distribution\n",
    "    ethnicity_counts = study_demog['Ethnicity simplified'].value_counts()\n",
    "    ethnicity_percentages = (ethnicity_counts / len(study_demog) * 100).round(1)\n",
    "    results['ethnicity'] = {\n",
    "        category: f\"{count} ({percentage}%)\"\n",
    "        for category, count, percentage in zip(\n",
    "            ethnicity_counts.index,\n",
    "            ethnicity_counts.values,\n",
    "            ethnicity_percentages.values\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Generate formatted output\n",
    "    output = f\"\"\"Demographic Characteristics (Total N = {results['n_participants']})\n",
    "Available Demographic Data for {results['n_with_demographics']} participants ({round(results['n_with_demographics']/results['n_participants']*100, 1)}% of sample)\n",
    "\n",
    "Age (n = {results['age']['n']})\"\"\"\n",
    "    \n",
    "    if results['age']['mean'] is not None:\n",
    "        output += f\"\"\"\n",
    "  Mean (SD): {results['age']['mean']} ({results['age']['std']}) years\n",
    "  Range: {results['age']['range']} years\"\"\"\n",
    "    \n",
    "    if results['age']['missing'] > 0:\n",
    "        output += f\"\\n  Missing: {results['age']['missing']}\"\n",
    "    \n",
    "    output += f\"\"\"\n",
    "\n",
    "Sex\n",
    "{chr(10).join(f\"  {category}: {count}\" for category, count in results['sex'].items())}\n",
    "\n",
    "Ethnicity\n",
    "{chr(10).join(f\"  {category}: {count}\" for category, count in results['ethnicity'].items())}\"\"\"\n",
    "    \n",
    "    return output, results\n",
    "\n",
    "\n",
    "def demog_results_to_latex(results):\n",
    "    \"\"\"\n",
    "    Convert demographic analysis results to a LaTeX table with specific formatting requirements.\n",
    "    \n",
    "    Parameters:\n",
    "    results (dict): Results dictionary from demographic_analysis function\n",
    "    \n",
    "    Returns:\n",
    "    str: LaTeX formatted table\n",
    "    \"\"\"\n",
    "    def clean_value(value):\n",
    "        \"\"\"Helper function to clean values\"\"\"\n",
    "        value = str(value)\n",
    "        value = value.replace(\"%\", \"\\\\%\") # Ensure all percentages use escaped character\n",
    "        return value\n",
    "    \n",
    "    # Calculate percentage of sample with demographics\n",
    "    n_with_demos = results['n_with_demographics']\n",
    "    total_n = results['n_participants']\n",
    "    demo_percent = round(n_with_demos/total_n*100, 1)\n",
    "    \n",
    "    latex = r\"\"\"\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\begin{tabular}{ll}\"\"\"\n",
    "    \n",
    "    # Total participants and demographic coverage\n",
    "    latex += f\"\\nTotal participants & {total_n} \\\\\\\\\"\n",
    "    latex += f\"\\nPts. w/ demographic data & {n_with_demos} ({demo_percent}\\\\%) \\\\\\\\\"\n",
    "    \n",
    "    # Age section\n",
    "    if results['age']['mean'] is not None:\n",
    "        latex += \"\\nAge & \\\\\\\\\"\n",
    "        latex += f\"\\quad Mean (SD) & {results['age']['mean']} ({results['age']['std']}) years \\\\\\\\\"\n",
    "        latex += f\"\\quad Range & {results['age']['range']} years \\\\\\\\\"\n",
    "        if results['age']['missing'] > 0:\n",
    "            missing_percent = round(results['age']['missing']/n_with_demos*100, 1)\n",
    "            latex += f\"\\quad Missing & {results['age']['missing']} ({missing_percent}\\\\%) \\\\\\\\\"\n",
    "    \n",
    "    # Sex section\n",
    "    latex += \"\\nSex & \\\\\\\\\"\n",
    "    for category, value in results['sex'].items():\n",
    "        category = clean_value(category)\n",
    "        value = clean_value(value)\n",
    "        latex += f\"\\quad {category} & {value} \\\\\\\\\"\n",
    "    \n",
    "    # Ethnicity section\n",
    "    latex += \"\\nEthnicity & \\\\\\\\\"\n",
    "    for category, value in results['ethnicity'].items():\n",
    "        category = clean_value(category)\n",
    "        value = clean_value(value)\n",
    "        latex += f\"\\quad {category} & {value} \\\\\\\\\"\n",
    "    \n",
    "    # Close the table\n",
    "    latex += r\"\"\"\n",
    "\\end{tabular}\n",
    "\\caption{Demographic Characteristics of Study Participants}\n",
    "\\label{tab:demographics}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "    \n",
    "    return latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demographics for all of the experiments combined\n",
    "# You can generate an experiment-specific table with (for example) \"demog_output, demog_results = demographic_analysis(df_idaea4, demog_idaea4)\"\n",
    "\n",
    "if DEIDENTIFIED_DATA:\n",
    "  print(\"Demographic tables cannot be re-generated from the de-identified dataset. Please see the demographics table in the paper.\")\n",
    "else:\n",
    "  # Load ImageNet16 demographics\n",
    "  demog_imagenet16 = pd.read_csv(\"results/demographics_imagenet16_v1_mod_2.csv\")\n",
    "  demog_imagenet16 = demog_imagenet16[demog_imagenet16[\"Status\"] == \"APPROVED\"]\n",
    "\n",
    "  demog_imagenet16_loss_ablation = pd.read_csv(\"results/demographics_imagenet16_v1_mod_1.csv\")\n",
    "  demog_imagenet16_loss_ablation = demog_imagenet16_loss_ablation[demog_imagenet16_loss_ablation[\"Status\"] == \"APPROVED\"]\n",
    "\n",
    "  demog_imagenet16_guide_comparison = pd.read_csv(\"results/demographics_imagenet16_v1_mod_4.csv\")\n",
    "  demog_imagenet16_guide_comparison = demog_imagenet16_guide_comparison[demog_imagenet16_guide_comparison[\"Status\"] == \"APPROVED\"]\n",
    "\n",
    "  # Load idaea4 demographics\n",
    "  demog_idaea4_learn_1 = pd.read_csv(\"results/demographics_idaea4_learn_1.csv\")\n",
    "  demog_idaea4_learn_2 = pd.read_csv(\"results/demographics_idaea4_learn_2.csv\")\n",
    "  demog_idaea4 = pd.concat([demog_idaea4_learn_1, demog_idaea4_learn_2])\n",
    "  demog_idaea4 = demog_idaea4[demog_idaea4[\"Status\"] == \"APPROVED\"]\n",
    "  assert demog_idaea4[\"Participant id\"].is_unique, \"There are duplicate Participant id (worker_id) values in the dataframe\"\n",
    "\n",
    "  # Load HAM4 demographics\n",
    "  demog_ham4_learn_4 = pd.read_csv(\"results/demographics_ham4_learn_4.csv\")\n",
    "  demog_ham4_learn_5 = pd.read_csv(\"results/demographics_ham4_learn_5.csv\")\n",
    "  demog_ham4 = pd.concat([demog_ham4_learn_4, demog_ham4_learn_5])\n",
    "  demog_ham4 = demog_ham4[demog_ham4[\"Status\"] == \"APPROVED\"]\n",
    "  assert demog_ham4[\"Participant id\"].is_unique, \"There are duplicate Participant id (worker_id) values in the dataframe\"\n",
    "\n",
    "  # Load MHIST demographics\n",
    "  demog_mhist = pd.read_csv(\"results/demographics_mhist_learn_1.csv\")\n",
    "  demog_mhist = demog_mhist[demog_mhist[\"Status\"] == \"APPROVED\"]\n",
    "  assert demog_mhist[\"Participant id\"].is_unique, \"There are duplicate Participant id (worker_id) values in the dataframe\"\n",
    "\n",
    "  # Combine demographics across experiments\n",
    "  demog_all = pd.concat([demog_imagenet16, demog_imagenet16_loss_ablation, demog_imagenet16_guide_comparison, demog_idaea4, demog_ham4, demog_mhist])\n",
    "\n",
    "  demog_unique = demog_all.drop_duplicates(subset=['Participant id'], keep='first')\n",
    "\n",
    "  worker_ids_all = pd.concat([df_main_i16[[\"worker_id\"]], df_loss_ablation_i16[[\"worker_id\"]], df_backbone_compare_i16[[\"worker_id\"]], df_idaea4[[\"worker_id\"]], df_ham4[[\"worker_id\"]], df_mhist[[\"worker_id\"]]])\n",
    "\n",
    "  demog_output, demog_results = demographic_analysis(worker_ids_all, demog_unique)\n",
    "\n",
    "  print(demog_output)\n",
    "  print(\"----------\")\n",
    "  print(demog_results_to_latex(demog_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raevenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
